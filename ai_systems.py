import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import numpy as np
from enum import Enum

class ModelType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ"""
    LSTM = "lstm"
    ISOLATION_FOREST = "isolation_forest"
    ONE_CLASS_SVM = "one_class_svm"
    MONTE_CARLO = "monte_carlo"
    GRADIENT_BOOSTING = "gradient_boosting"
    LIGHT_GBM = "light_gbm"

class ModelPerformance:
    """ØªØªØ¨Ø¹ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø°ÙƒÙŠ"""
    
    def __init__(self, model_type: ModelType):
        self.model_type = model_type
        self.accuracy_history: List[float] = []
        self.latency_history: List[float] = []
        self.memory_usage_history: List[float] = []
        self.last_used: Optional[datetime] = None
        self.success_count = 0
        self.failure_count = 0
    
    def update_performance(self, accuracy: float, latency: float, 
                         memory_usage: float, success: bool):
        """ØªØ­Ø¯ÙŠØ« Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        self.accuracy_history.append(accuracy)
        self.latency_history.append(latency)
        self.memory_usage_history.append(memory_usage)
        self.last_used = datetime.now()
        
        if success:
            self.success_count += 1
        else:
            self.failure_count += 1
        
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ ÙÙ‚Ø· Ø¨Ù€ 100 Ù‚ÙŠØ§Ø³ Ø­Ø¯ÙŠØ«Ø©
        if len(self.accuracy_history) > 100:
            self.accuracy_history = self.accuracy_history[-100:]
            self.latency_history = self.latency_history[-100:]
            self.memory_usage_history = self.memory_usage_history[-100:]
    
    def get_performance_score(self) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if not self.accuracy_history:
            return 0.0
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¬Ø­
        accuracy = np.mean(self.accuracy_history[-10:]) if self.accuracy_history else 0.0
        latency = np.mean(self.latency_history[-10:]) if self.latency_history else 1.0
        memory = np.mean(self.memory_usage_history[-10:]) if self.memory_usage_history else 1.0
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ…ÙˆÙ† ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø© Ø¥Ù„Ù‰ Ø¹ÙˆØ§Ù…Ù„ ØªØµØ­ÙŠØ­ (ÙƒÙ„Ù…Ø§ Ù‚Ù„ÙˆØ§ ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø£ÙØ¶Ù„)
        latency_factor = 1.0 / max(0.1, latency)
        memory_factor = 1.0 / max(0.1, memory)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        score = accuracy * 0.6 + latency_factor * 0.2 + memory_factor * 0.2
        return max(0.0, min(1.0, score))

class DynamicModelSelector:
    """
    Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ - Ù…ÙŠØ²Ø© Ø¨Ø±Ø§Ø¡Ø© Ø§Ù„Ø§Ø®ØªØ±Ø§Ø¹
    
    ÙŠØ®ØªØ§Ø± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰:
    - Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ
    - Ù‚ÙŠÙˆØ¯ Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©
    - Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©
    - Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
    
    ÙŠØ¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ ØªØ¯Ø®Ù„ Ø¨Ø´Ø±ÙŠ ÙˆÙŠØ­Ø³Ù† Ù†ÙØ³Ù‡ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.available_models: Dict[ModelType, Any] = {}
        self.model_performance: Dict[ModelType, ModelPerformance] = {}
        self.current_model: Optional[ModelType] = None
        self.logger = logging.getLogger(__name__)
        
        self.initialize_models()
    
    def initialize_models(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        for model_type in ModelType:
            self.model_performance[model_type] = ModelPerformance(model_type)
        
        self.logger.info("âœ… Dynamic Model Selector initialized")
    
    def register_model(self, model_type: ModelType, model_instance: Any):
        """ØªØ³Ø¬ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        self.available_models[model_type] = model_instance
        self.logger.info(f"âœ… Registered model: {model_type.value}")
    
    def select_best_model(self, data_characteristics: Dict[str, Any], 
                        resource_constraints: Dict[str, float]) -> ModelType:
        """
        Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯
        
        Args:
            data_characteristics: Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©
            resource_constraints: Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠØ© (CPU, RAM, etc.)
            
        Returns:
            ModelType: Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        """
        # ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data_complexity = self._analyze_data_complexity(data_characteristics)
        data_size = data_characteristics.get('size', 0)
        
        # ØªØ­Ù„ÙŠÙ„ Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        cpu_available = resource_constraints.get('cpu_available', 1.0)
        memory_available = resource_constraints.get('memory_available', 1.0)
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ù„Ø§Ø¦Ù…Ø© Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
        model_scores = {}
        
        for model_type, performance in self.model_performance.items():
            if model_type not in self.available_models:
                continue
            
            # Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ
            performance_score = performance.get_performance_score()
            
            # Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ù„Ø§Ø¦Ù…Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            suitability_score = self._calculate_model_suitability(
                model_type, data_complexity, data_size
            )
            
            # Ø¯Ø±Ø¬Ø© ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
            efficiency_score = self._calculate_resource_efficiency(
                model_type, cpu_available, memory_available
            )
            
            # Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Ù…Ø±Ø¬Ø­Ø©)
            final_score = (
                performance_score * 0.4 +
                suitability_score * 0.3 +
                efficiency_score * 0.3
            )
            
            model_scores[model_type] = final_score
        
        if not model_scores:
            self.logger.warning("No models available, using default")
            return ModelType.LSTM
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø£Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø©
        best_model = max(model_scores.items(), key=lambda x: x[1])[0]
        self.current_model = best_model
        
        self.logger.info(f"ğŸ¯ Selected model: {best_model.value} (score: {model_scores[best_model]:.3f})")
        return best_model
    
    def _analyze_data_complexity(self, data_characteristics: Dict[str, Any]) -> float:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        size = data_characteristics.get('size', 0)
        dimensions = data_characteristics.get('dimensions', 0)
        variability = data_characteristics.get('variability', 0)
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ (0-1)
        complexity = min(1.0, (size / 10000 + dimensions / 10 + variability) / 3)
        return complexity
    
    def _calculate_model_suitability(self, model_type: ModelType, 
                                   data_complexity: float, data_size: int) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ù„Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        suitability_scores = {
            ModelType.LSTM: min(1.0, 0.8 + data_complexity * 0.2),  # Ø¬ÙŠØ¯ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
            ModelType.ISOLATION_FOREST: 0.9 if data_size > 1000 else 0.6,
            ModelType.ONE_CLASS_SVM: 0.7 + data_complexity * 0.3,
            ModelType.MONTE_CARLO: 0.8 if data_complexity < 0.5 else 0.6,
            ModelType.GRADIENT_BOOSTING: 0.9 if data_size > 500 else 0.7,
            ModelType.LIGHT_GBM: 0.85 if data_size > 1000 else 0.65
        }
        
        return suitability_scores.get(model_type, 0.5)
    
    def _calculate_resource_efficiency(self, model_type: ModelType,
                                    cpu_available: float, memory_available: float) -> float:
        """Ø­Ø³Ø§Ø¨ ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        # Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠØ© Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
        resource_needs = {
            ModelType.LSTM: {'cpu': 0.8, 'memory': 0.7},
            ModelType.ISOLATION_FOREST: {'cpu': 0.5, 'memory': 0.4},
            ModelType.ONE_CLASS_SVM: {'cpu': 0.6, 'memory': 0.5},
            ModelType.MONTE_CARLO: {'cpu': 0.9, 'memory': 0.6},
            ModelType.GRADIENT_BOOSTING: {'cpu': 0.7, 'memory': 0.5},
            ModelType.LIGHT_GBM: {'cpu': 0.6, 'memory': 0.4}
        }
        
        needs = resource_needs.get(model_type, {'cpu': 0.7, 'memory': 0.5})
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙƒÙØ§Ø¡Ø© (ÙƒÙ„Ù…Ø§ Ù‚Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„Ù…Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙØ§Ø¡Ø© Ø£Ø¹Ù„Ù‰)
        cpu_efficiency = 1.0 - max(0, needs['cpu'] - cpu_available)
        memory_efficiency = 1.0 - max(0, needs['memory'] - memory_available)
        
        return (cpu_efficiency + memory_efficiency) / 2
    
    def update_model_performance(self, model_type: ModelType, accuracy: float,
                               latency: float, memory_usage: float, success: bool):
        """ØªØ­Ø¯ÙŠØ« Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        if model_type in self.model_performance:
            self.model_performance[model_type].update_performance(
                accuracy, latency, memory_usage, success
            )
    
    def get_performance_report(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        report = {}
        for model_type, performance in self.model_performance.items():
            report[model_type.value] = {
                'performance_score': performance.get_performance_score(),
                'success_rate': performance.success_count / max(1, performance.success_count + performance.failure_count),
                'last_used': performance.last_used.isoformat() if performance.last_used else None,
                'usage_count': performance.success_count + performance.failure_count
            }
        return report
    
        def auto_optimize(self):
        """Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø°Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù†Ø®ÙØ¶
            for model_type, performance in self.model_performance.items():
                if performance.get_performance_score() < 0.6:
                    self._retrain_model(model_type)
            
            # ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø¬Ø¯ÙŠØ¯Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ØªØ³Ù…Ø­
            if self._check_resource_availability():
                self._load_additional_models()
                
            self.logger.info("âœ… Model auto-optimization completed")
            
        except Exception as e:
            self.logger.error(f"âŒ Auto-optimization failed: {e}")
    
    def _retrain_model(self, model_type: ModelType):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if model_type in self.available_models:
            try:
                model = self.available_models[model_type]
                if hasattr(model, 'retrain'):
                    model.retrain()
                    self.logger.info(f"âœ… Retrained model: {model_type.value}")
            except Exception as e:
                self.logger.error(f"âŒ Failed to retrain {model_type.value}: {e}")
    
    def _check_resource_availability(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø¶Ø§ÙÙŠØ©"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        return True  # ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† CPU Ùˆ RAM
    
    def _load_additional_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ØªØ³Ù…Ø­"""
        # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù†Ù…Ø§Ø°Ø¬ Ø¬Ø¯ÙŠØ¯Ø© Ù‡Ù†Ø§
        pass

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø©
def create_model_selector(config: Dict[str, Any]) -> DynamicModelSelector:
    return DynamicModelSelector(config)
