import logging
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import joblib
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import precision_score, recall_score, f1_score
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import warnings
import hashlib
import json
import math
from scipy import stats
from dataclasses import dataclass
from enum import Enum
import torch.nn.functional as F

warnings.filterwarnings('ignore')

class ModelType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
    ISOLATION_FOREST = "isolation_forest"
    ONE_CLASS_SVM = "one_class_svm"
    AUTOENCODER = "autoencoder"
    LSTM = "lstm"
    TRANSFORMER = "transformer"

@dataclass
class ModelPerformance:
    """Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    training_loss: List[float]
    validation_loss: List[float]
    last_trained: datetime

class DynamicModelSelector:
    """Ù…Ø®ØªØ§Ø± Ù†Ù…Ø§Ø°Ø¬ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('SmartNeural.AI.ModelSelector')
        self.available_models = {}
        self.model_performance = {}
        self.current_best_model = None
        
        self._initialize_models()
        self.logger.info("âœ… Dynamic Model Selector Initialized - SS Rating")
    
    def _initialize_models(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        try:
            # Ù†Ù…Ø§Ø°Ø¬ ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
            self.available_models[ModelType.ISOLATION_FOREST] = {
                'model': IsolationForest(
                    n_estimators=200,
                    contamination=0.1,
                    max_features=1.0,
                    bootstrap=True,
                    random_state=42,
                    n_jobs=-1
                ),
                'weight': 0.3,
                'performance': ModelPerformance(0.0, 0.0, 0.0, 0.0, [], [], datetime.now())
            }
            
            self.available_models[ModelType.ONE_CLASS_SVM] = {
                'model': OneClassSVM(
                    nu=0.1,
                    kernel='rbf',
                    gamma='scale'
                ),
                'weight': 0.2,
                'performance': ModelPerformance(0.0, 0.0, 0.0, 0.0, [], [], datetime.now())
            }
            
            self.available_models[ModelType.AUTOENCODER] = {
                'model': self._build_advanced_autoencoder(),
                'weight': 0.3,
                'performance': ModelPerformance(0.0, 0.0, 0.0, 0.0, [], [], datetime.now())
            }
            
            self.available_models[ModelType.LSTM] = {
                'model': self._build_lstm_anomaly_detector(),
                'weight': 0.2,
                'performance': ModelPerformance(0.0, 0.0, 0.0, 0.0, [], [], datetime.now())
            }
            
            self.logger.info(f"âœ… {len(self.available_models)} AI models initialized")
            
        except Exception as e:
            self.logger.error(f"âŒ Model initialization failed: {e}")
    
    def _build_advanced_autoencoder(self) -> nn.Module:
        """Ø¨Ù†Ø§Ø¡ Autoencoder Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ°"""
        class AdvancedAutoencoder(nn.Module):
            def __init__(self, input_dim=6, encoding_dim=32):
                super().__init__()
                # Encoder Ù…ØªÙ‚Ø¯Ù…
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, 128),
                    nn.ReLU(),
                    nn.BatchNorm1d(128),
                    nn.Dropout(0.2),
                    nn.Linear(128, 64),
                    nn.ReLU(),
                    nn.BatchNorm1d(64),
                    nn.Dropout(0.2),
                    nn.Linear(64, encoding_dim),
                    nn.ReLU(),
                    nn.BatchNorm1d(encoding_dim)
                )
                # Decoder Ù…ØªÙ‚Ø¯Ù…
                self.decoder = nn.Sequential(
                    nn.Linear(encoding_dim, 64),
                    nn.ReLU(),
                    nn.BatchNorm1d(64),
                    nn.Dropout(0.2),
                    nn.Linear(64, 128),
                    nn.ReLU(),
                    nn.BatchNorm1d(128),
                    nn.Dropout(0.2),
                    nn.Linear(128, input_dim),
                    nn.Tanh()
                )
            
            def forward(self, x):
                encoded = self.encoder(x)
                decoded = self.decoder(encoded)
                return decoded
        
        return AdvancedAutoencoder(
            input_dim=6,
            encoding_dim=self.config['ai_models']['autoencoder']['encoding_dim']
        )
    
    def _build_lstm_anomaly_detector(self) -> nn.Module:
        """Ø¨Ù†Ø§Ø¡ LSTM Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ°"""
        class LSTMAutoencoder(nn.Module):
            def __init__(self, input_size=6, hidden_size=64, num_layers=2):
                super().__init__()
                self.hidden_size = hidden_size
                self.num_layers = num_layers
                
                # LSTM Encoder
                self.encoder = nn.LSTM(input_size, hidden_size, num_layers, 
                                     batch_first=True, dropout=0.2)
                
                # LSTM Decoder
                self.decoder = nn.LSTM(hidden_size, hidden_size, num_layers,
                                     batch_first=True, dropout=0.2)
                
                self.output_layer = nn.Linear(hidden_size, input_size)
            
            def forward(self, x):
                # Encoding
                _, (hidden, cell) = self.encoder(x)
                
                # Decoding
                decoder_input = torch.zeros(x.size(0), x.size(1), self.hidden_size).to(x.device)
                decoder_output, _ = self.decoder(decoder_input, (hidden, cell))
                
                # Reconstruction
                output = self.output_layer(decoder_output)
                return output
        
        return LSTMAutoencoder()

class SensorFeatureMapper:
    """Ù…Ø¯ÙŠØ± Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ´Ø¹Ø±Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, feature_names: List[str]):
        self.feature_names = sorted(feature_names)
        self.feature_indices = {name: idx for idx, name in enumerate(self.feature_names)}
        self.feature_importance = {name: 1.0 for name in feature_names}  # Ø£Ù‡Ù…ÙŠØ© ÙƒÙ„ Ù…ÙŠØ²Ø©
    
    def extract_ordered_features(self, sensor_data: Dict[str, Any]) -> np.ndarray:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨ØªØ±ØªÙŠØ¨ Ø«Ø§Ø¨Øª Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©"""
        return np.array([sensor_data.get(sensor, 0.0) for sensor in self.feature_names])
    
    def update_feature_importance(self, importances: Dict[str, float]):
        """ØªØ­Ø¯ÙŠØ« Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        for feature, importance in importances.items():
            if feature in self.feature_importance:
                self.feature_importance[feature] = importance
    
    def get_weighted_features(self, sensor_data: Dict[str, Any]) -> np.ndarray:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙŠØ²Ø§Øª Ù…ÙˆØ²ÙˆÙ†Ø© Ø¨Ø§Ù„Ø£Ù‡Ù…ÙŠØ©"""
        features = self.extract_ordered_features(sensor_data)
        weights = np.array([self.feature_importance[feature] for feature in self.feature_names])
        return features * weights

class AdvancedPreprocessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - SS Rating"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('SmartNeural.AI.Preprocessor')
        
        # Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø³ÙƒØ§Ù„Ø±Ø§Øª Ù„Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.scalers = {
            'standard': StandardScaler(),
            'minmax': MinMaxScaler(),
            'robust': StandardScaler()  # Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ù„Ù€ RobustScaler
        }
        
        self.feature_mapper = SensorFeatureMapper([
            'pressure', 'temperature', 'methane', 'hydrogen_sulfide', 'vibration', 'flow'
        ])
        
        self.is_fitted = False
        self.data_statistics = {}
        self.feature_correlations = {}
        
        self.logger.info("âœ… Advanced Preprocessor Initialized - SS Rating")
    
    def fit(self, training_data: List[Dict[str, Any]]):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø´ÙƒÙ„ Ù…ØªÙ‚Ø¯Ù…"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­
            X = np.array([self.feature_mapper.extract_ordered_features(data) for data in training_data])
            
            # Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self._compute_data_statistics(X)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ÙŠØ²Ø§Øª
            self._compute_feature_correlations(X)
            
            # ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø³ÙƒØ§Ù„Ø±Ø§Øª
            for scaler_name, scaler in self.scalers.items():
                scaler.fit(X)
            
            self.is_fitted = True
            self.logger.info("âœ… Advanced preprocessor fitted successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ Preprocessor fitting failed: {e}")
    
    def _compute_data_statistics(self, X: np.ndarray):
        """Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        self.data_statistics = {
            'mean': np.mean(X, axis=0),
            'std': np.std(X, axis=0),
            'min': np.min(X, axis=0),
            'max': np.max(X, axis=0),
            'median': np.median(X, axis=0),
            'skewness': stats.skew(X, axis=0),
            'kurtosis': stats.kurtosis(X, axis=0)
        }
    
    def _compute_feature_correlations(self, X: np.ndarray):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        df = pd.DataFrame(X, columns=self.feature_mapper.feature_names)
        self.feature_correlations = df.corr().to_dict()
    
    def preprocess_data(self, sensor_data: Dict[str, Any]) -> np.ndarray:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ´Ø¹Ø±Ø§Øª"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚ÙŠÙ… Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­
            values = self.feature_mapper.extract_ordered_features(sensor_data)
            array_data = values.reshape(1, -1)
            
            if self.is_fitted:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙƒØ§Ù„Ø± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                normalized_data = self.scalers['minmax'].transform(array_data)
            else:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ·Ø¨ÙŠØ¹ Ø¨Ø³ÙŠØ· Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨
                normalized_data = array_data / np.array([200, 300, 5000, 500, 20, 500])
            
            return normalized_data
            
        except Exception as e:
            self.logger.error(f"âŒ Advanced data preprocessing failed: {e}")
            return np.zeros((1, len(self.feature_mapper.feature_names)))
    
    def detect_data_quality_issues(self, sensor_data: Dict[str, Any]) -> List[str]:
        """ÙƒØ´Ù Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        issues = []
        values = self.feature_mapper.extract_ordered_features(sensor_data)
        
        for i, (feature, value) in enumerate(zip(self.feature_mapper.feature_names, values)):
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©
            if self.data_statistics and self.is_fitted:
                z_score = abs(value - self.data_statistics['mean'][i]) / (self.data_statistics['std'][i] + 1e-8)
                if z_score > 3:  # Ø®Ø§Ø±Ø¬ 3 Ø§Ù†Ø­Ø±Ø§ÙØ§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ©
                    issues.append(f"Outlier detected in {feature}: z-score = {z_score:.2f}")
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ­ÙŠÙ„Ø©
            sensor_config = self.config['sensors'].get(feature, {})
            if value < sensor_config.get('min', 0) or value > sensor_config.get('max', 1000):
                issues.append(f"Impossible value in {feature}: {value}")
        
        return issues
    
    def create_sequences(self, data: np.ndarray, sequence_length: int = 50) -> np.ndarray:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªÙˆØ§Ù„ÙŠØ§Øª Ø²Ù…Ù†ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        sequences = []
        for i in range(len(data) - sequence_length):
            seq = data[i:(i + sequence_length)]
            sequences.append(seq)
        return np.array(sequences)

class EnsembleAnomalyDetector:
    """ÙƒØ§Ø´Ù Ø´Ø°ÙˆØ° Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¬Ù…ÙŠØ¹"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('SmartNeural.AI.EnsembleAnomaly')
        
        self.models = {}
        self.model_weights = {}
        self.performance_history = {}
        
        self._initialize_ensemble_models()
        self.logger.info("âœ… Ensemble Anomaly Detector Initialized - SS Rating")
    
    def _initialize_ensemble_models(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹"""
        # Isolation Forest Ù…Ø­Ø³Ù†
        self.models['isolation_forest'] = IsolationForest(
            n_estimators=300,
            contamination=0.05,  # Ø£ÙƒØ«Ø± ØªØ­ÙØ¸Ø§Ù‹
            max_features=0.8,
            bootstrap=True,
            random_state=42,
            n_jobs=-1
        )
        
        # One-Class SVM Ù…Ø­Ø³Ù†
        self.models['one_class_svm'] = OneClassSVM(
            nu=0.05,  # Ø£ÙƒØ«Ø± ØªØ­ÙØ¸Ø§Ù‹
            kernel='rbf',
            gamma='auto',
            cache_size=1000
        )
        
        # Local Outlier Factor
        from sklearn.neighbors import LocalOutlierFactor
        self.models['lof'] = LocalOutlierFactor(
            n_neighbors=20,
            contamination=0.1,
            novelty=True
        )
        
        # ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        self.model_weights = {
            'isolation_forest': 0.4,
            'one_class_svm': 0.3,
            'lof': 0.3
        }
    
    def train_ensemble(self, X: np.ndarray):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹"""
        try:
            for name, model in self.models.items():
                if hasattr(model, 'fit'):
                    model.fit(X)
                    self.logger.info(f"âœ… Ensemble model {name} trained successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ Ensemble training failed: {e}")
    
    def predict_ensemble(self, X: np.ndarray) -> Dict[str, Any]:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            predictions = {}
            scores = {}
            
            for name, model in self.models.items():
                if hasattr(model, 'predict'):
                    pred = model.predict(X)
                    predictions[name] = (pred == -1)  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ boolean
                
                if hasattr(model, 'decision_function'):
                    score = model.decision_function(X)
                    scores[name] = score
                elif hasattr(model, 'score_samples'):
                    score = model.score_samples(X)
                    scores[name] = score
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
            weighted_score = 0.0
            total_weight = 0.0
            
            for name, weight in self.model_weights.items():
                if name in predictions:
                    weighted_score += weight * (1.0 if predictions[name] else 0.0)
                    total_weight += weight
            
            final_prediction = weighted_score / total_weight if total_weight > 0 else 0.0
            
            return {
                'ensemble_score': final_prediction,
                'individual_predictions': predictions,
                'individual_scores': scores,
                'is_anomaly': final_prediction > 0.6,
                'confidence': min(1.0, abs(final_prediction - 0.5) * 2)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Ensemble prediction failed: {e}")
            return {'is_anomaly': False, 'ensemble_score': 0.0, 'confidence': 0.0}

class AdvancedAnomalySystem:
    """Ù†Ø¸Ø§Ù… ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - SS Rating"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('SmartNeural.AI.Anomaly')
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self.preprocessor = AdvancedPreprocessor(config)
        self.ensemble_detector = EnsembleAnomalyDetector(config)
        self.autoencoder = self._build_advanced_autoencoder()
        
        # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ù„Ø©
        self.anomaly_history = []
        self.performance_metrics = {}
        self.is_trained = False
        
        # Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ
        self.adaptive_threshold = 0.6
        self.false_positive_rate = 0.0
        self.detection_sensitivity = 0.8
        
        self.logger.info("âœ… Advanced Anomaly Detection System Initialized - SS Rating")
    
    def _build_advanced_autoencoder(self) -> nn.Module:
        """Ø¨Ù†Ø§Ø¡ Autoencoder Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ØªØ¹Ù‚ÙŠØ¯ Ù…Ù†Ø§Ø³Ø¨"""
        class AdvancedAutoencoder(nn.Module):
            def __init__(self, input_dim=6, encoding_dim=32):
                super().__init__()
                
                # Encoder Ù…Ø¹ ØªØ³Ø±Ø¨ ÙˆØ·Ø¨Ù‚Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, 128),
                    nn.LeakyReLU(0.1),
                    nn.BatchNorm1d(128),
                    nn.Dropout(0.3),
                    nn.Linear(128, 64),
                    nn.LeakyReLU(0.1),
                    nn.BatchNorm1d(64),
                    nn.Dropout(0.2),
                    nn.Linear(64, encoding_dim),
                    nn.Tanh()
                )
                
                # Decoder Ù…ØªÙ†Ø§Ø¸Ø±
                self.decoder = nn.Sequential(
                    nn.Linear(encoding_dim, 64),
                    nn.LeakyReLU(0.1),
                    nn.BatchNorm1d(64),
                    nn.Dropout(0.2),
                    nn.Linear(64, 128),
                    nn.LeakyReLU(0.1),
                    nn.BatchNorm1d(128),
                    nn.Dropout(0.3),
                    nn.Linear(128, input_dim),
                    nn.Tanh()
                )
            
            def forward(self, x):
                encoded = self.encoder(x)
                decoded = self.decoder(encoded)
                return decoded
        
        return AdvancedAutoencoder(
            input_dim=6,
            encoding_dim=self.config['ai_models']['autoencoder']['encoding_dim']
        )
    
    def train_models(self, training_data: List[Dict[str, Any]]):
        """ØªØ¯Ø±ÙŠØ¨ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        try:
            self.logger.info("ğŸ”„ Training advanced anomaly detection models...")
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø£ÙˆÙ„Ø§Ù‹
            self.preprocessor.fit(training_data)
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X = np.array([self.preprocessor.feature_mapper.extract_ordered_features(data) 
                         for data in training_data])
            
            # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹
            self.ensemble_detector.train_ensemble(X)
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù€Autoencoder
            self._train_autoencoder(X)
            
            # Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø¹ØªØ¨Ø§Øª
            self._calibrate_detection_thresholds(X)
            
            self.is_trained = True
            self.logger.info("âœ… All advanced anomaly models trained successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ Advanced anomaly models training failed: {e}")
    
    def _train_autoencoder(self, X: np.ndarray):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù€Autoencoder Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_normalized = self.preprocessor.scalers['minmax'].transform(X)
            X_tensor = torch.FloatTensor(X_normalized)
            dataset = TensorDataset(X_tensor)
            dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
            
            # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            criterion = nn.MSELoss()
            optimizer = optim.AdamW(self.autoencoder.parameters(), 
                                  lr=0.001, weight_decay=1e-5)
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)
            
            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
            self.autoencoder.train()
            training_losses = []
            
            for epoch in range(self.config['ai_models']['autoencoder']['epochs']):
                epoch_loss = 0.0
                for batch in dataloader:
                    data = batch[0]
                    optimizer.zero_grad()
                    reconstructed = self.autoencoder(data)
                    loss = criterion(reconstructed, data)
                    loss.backward()
                    
                    # Gradient Clipping Ù„Ù…Ù†Ø¹ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø±
                    torch.nn.utils.clip_grad_norm_(self.autoencoder.parameters(), 1.0)
                    optimizer.step()
                    epoch_loss += loss.item()
                
                avg_loss = epoch_loss / len(dataloader)
                training_losses.append(avg_loss)
                scheduler.step(avg_loss)
                
                if epoch % 25 == 0:
                    self.logger.info(f"Autoencoder Epoch {epoch}, Loss: {avg_loss:.6f}")
                    
            self.performance_metrics['autoencoder_training_loss'] = training_losses
            
        except Exception as e:
            self.logger.error(f"âŒ Advanced autoencoder training failed: {e}")
    
    def _calibrate_detection_thresholds(self, X: np.ndarray):
        """Ù…Ø¹Ø§ÙŠØ±Ø© Ø¹ØªØ¨Ø§Øª Ø§Ù„ÙƒØ´Ù Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        try:
            # Ø­Ø³Ø§Ø¨ Ø£Ø®Ø·Ø¥ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ù†Ø§Ø¡ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            reconstruction_errors = []
            self.autoencoder.eval()
            
            with torch.no_grad():
                X_normalized = self.preprocessor.scalers['minmax'].transform(X)
                X_tensor = torch.FloatTensor(X_normalized)
                
                for i in range(0, len(X_tensor), 32):
                    batch = X_tensor[i:i+32]
                    reconstructed = self.autoencoder(batch)
                    error = torch.mean((batch - reconstructed) ** 2, dim=1)
                    reconstruction_errors.extend(error.numpy())
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹ØªØ¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ©
            reconstruction_errors = np.array(reconstruction_errors)
            self.adaptive_threshold = np.percentile(reconstruction_errors, 95)  % 95%
            
            self.logger.info(f"âœ… Detection threshold calibrated: {self.adaptive_threshold:.4f}")
            
        except Exception as e:
            self.logger.error(f"âŒ Threshold calibration failed: {e}")
            self.adaptive_threshold = 0.05  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¢Ù…Ù†Ø©
    
    def detect_anomalies(self, sensor_data: Dict[str, Any]) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø´Ø°ÙˆØ° Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©"""
        try:
            if not self.is_trained:
                return self._get_untrained_response()
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„Ø§Ù‹
            data_quality_issues = self.preprocessor.detect_data_quality_issues(sensor_data)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            processed_data = self.preprocessor.preprocess_data(sensor_data)
            
            # Ø§Ù„ÙƒØ´Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹
            ensemble_result = self.ensemble_detector.predict_ensemble(processed_data)
            
            # Ø§Ù„ÙƒØ´Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€Autoencoder
            autoencoder_result = self._detect_autoencoder_anomaly(processed_data)
            
            # ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø´Ø°ÙˆØ°
            critical_anomalies = self._analyze_critical_anomalies(sensor_data)
            temporal_analysis = self._perform_temporal_analysis(sensor_data)
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø´ÙƒÙ„ Ù…ØªÙ‚Ø¯Ù…
            final_score = self._fuse_detection_results(ensemble_result, autoencoder_result)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„ØªÙƒÙŠÙÙŠØ©
            self._update_adaptive_threshold(final_score)
            
            result = {
                'is_anomaly': final_score > self.adaptive_threshold,
                'anomaly_score': float(final_score),
                'risk_level': self._calculate_risk_level(final_score, critical_anomalies),
                'ensemble_results': ensemble_result,
                'autoencoder_result': autoencoder_result,
                'critical_anomalies': critical_anomalies,
                'temporal_analysis': temporal_analysis,
                'data_quality_issues': data_quality_issues,
                'confidence': self._calculate_advanced_confidence(ensemble_result, autoencoder_result),
                'adaptive_threshold': float(self.adaptive_threshold),
                'recommendations': self._generate_anomaly_recommendations(final_score, critical_anomalies),
                'timestamp': datetime.now()
            }
            
            # ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„Ø³Ø¬Ù„
            self._update_anomaly_history(result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Advanced anomaly detection failed: {e}")
            return {'error': str(e), 'is_anomaly': False, 'anomaly_score': 0.0}
    
    def _detect_autoencoder_anomaly(self, data: np.ndarray) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Autoencoder Ù…ØªÙ‚Ø¯Ù…"""
        try:
            self.autoencoder.eval()
            with torch.no_grad():
                data_tensor = torch.FloatTensor(data)
                reconstructed = self.autoencoder(data_tensor)
                
                # Ø­Ø³Ø§Ø¨ Ø®Ø·Ø¥ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ù†Ø§Ø¡ Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªØ¹Ø¯Ø¯Ø©
                mse_error = torch.mean((data_tensor - reconstructed) ** 2).item()
                mae_error = torch.mean(torch.abs(data_tensor - reconstructed)).item()
                
                # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
                reconstruction_error = 0.7 * mse_error + 0.3 * mae_error
                
                return {
                    'reconstruction_error': reconstruction_error,
                    'mse_error': mse_error,
                    'mae_error': mae_error,
                    'is_anomaly': reconstruction_error > self.adaptive_threshold,
                    'confidence': min(1.0, reconstruction_error / (self.adaptive_threshold * 2))
                }
                
        except Exception as e:
            self.logger.error(f"âŒ Autoencoder detection failed: {e}")
            return {'reconstruction_error': 0.0, 'is_anomaly': False, 'confidence': 0.0}
    
    def _fuse_detection_results(self, ensemble_result: Dict, autoencoder_result: Dict) -> float:
        """Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙƒØ´Ù Ø¨Ø´ÙƒÙ„ Ù…ØªÙ‚Ø¯Ù…"""
        try:
            ensemble_score = ensemble_result.get('ensemble_score', 0.0)
            autoencoder_score = autoencoder_result.get('reconstruction_error', 0.0)
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª
            ensemble_normalized = ensemble_score
            autoencoder_normalized = min(1.0, autoencoder_score / (self.adaptive_threshold * 2))
            
            # Ø¯Ù…Ø¬ Ù…Ø±Ø¬Ø­ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø«Ù‚Ø©
            ensemble_confidence = ensemble_result.get('confidence', 0.5)
            autoencoder_confidence = autoencoder_result.get('confidence', 0.5)
            
            total_confidence = ensemble_confidence + autoencoder_confidence
            if total_confidence > 0:
                weight_ensemble = ensemble_confidence / total_confidence
                weight_autoencoder = autoencoder_confidence / total_confidence
            else:
                weight_ensemble = weight_autoencoder = 0.5
            
            fused_score = (weight_ensemble * ensemble_normalized + 
                          weight_autoencoder * autoencoder_normalized)
            
            return min(1.0, fused_score)
            
        except Exception as e:
            self.logger.error(f"âŒ Results fusion failed: {e}")
            return 0.0
    
    def _analyze_critical_anomalies(self, sensor_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø­Ø±Ø¬"""
        critical_anomalies = []
        sensor_config = self.config['sensors']
        
        for sensor, value in sensor_data.items():
            if sensor in sensor_config:
                config = sensor_config[sensor]
                critical_threshold = config.get('critical', 100)
                warning_threshold = critical_threshold * 0.8
                
                if value >= critical_threshold:
                    critical_anomalies.append({
                        'sensor': sensor,
                        'value': value,
                        'threshold': critical_threshold,
                        'severity': 'CRITICAL',
                        'description': f'{sensor} exceeded critical threshold'
                    })
                elif value >= warning_threshold:
                    critical_anomalies.append({
                        'sensor': sensor,
                        'value': value,
                        'threshold': warning_threshold,
                        'severity': 'WARNING',
                        'description': f'{sensor} approaching critical threshold'
                    })
        
        return critical_anomalies
    
    def _perform_temporal_analysis(self, sensor_data: Dict[str, Any]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø²Ù…Ù†ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ù‚ØµÙŠØ±
            if len(self.anomaly_history) > 10:
                recent_scores = [entry['anomaly_score'] for entry in self.anomaly_history[-10:]]
                trend = np.polyfit(range(len(recent_scores)), recent_scores, 1)[0]
                
                return {
                    'short_term_trend': trend,
                    'trend_direction': 'increasing' if trend > 0.01 else 'decreasing' if trend < -0.01 else 'stable',
                    'volatility': np.std(recent_scores) if recent_scores else 0.0,
                    'stability_index': 1.0 - (np.std(recent_scores) if recent_scores else 0.0)
                }
            else:
                return {
                    'short_term_trend': 0.0,
                    'trend_direction': 'unknown',
                    'volatility': 0.0,
                    'stability_index': 0.5
                }
                
        except Exception as e:
            self.logger.error(f"âŒ Temporal analysis failed: {e}")
            return {}
    
    def _calculate_risk_level(self, anomaly_score: float, critical_anomalies: List[Dict]) -> str:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        critical_count = len([a for a in critical_anomalies if a['severity'] == 'CRITICAL'])
        warning_count = len([a for a in critical_anomalies if a['severity'] == 'WARNING'])
        
        risk_factor = anomaly_score + (critical_count * 0.3) + (warning_count * 0.1)
        
        if risk_factor >= 0.8 or critical_count >= 2:
            return 'CRITICAL'
        elif risk_factor >= 0.6 or critical_count >= 1:
            return 'HIGH'
        elif risk_factor >= 0.4 or warning_count >= 2:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _calculate_advanced_confidence(self, ensemble_result: Dict, autoencoder_result: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ù…ØªÙ‚Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ÙƒØ´Ù"""
        ensemble_confidence = ensemble_result.get('confidence', 0.5)
        autoencoder_confidence = autoencoder_result.get('confidence', 0.5)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§ØªØ³Ø§Ù‚ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        ensemble_anomaly = ensemble_result.get('is_anomaly', False)
        autoencoder_anomaly = autoencoder_result.get('is_anomaly', False)
        
        if ensemble_anomaly == autoencoder_anomaly:
            consistency_boost = 0.2  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø«Ù‚Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø§ØªØ³Ø§Ù‚
        else:
            consistency_boost = -0.1  # Ø®ÙØ¶ Ø§Ù„Ø«Ù‚Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù
        
        base_confidence = (ensemble_confidence + autoencoder_confidence) / 2
        final_confidence = max(0.0, min(1.0, base_confidence + consistency_boost))
        
        return final_confidence
    
    def _generate_anomaly_recommendations(self, anomaly_score: float, critical_anomalies: List[Dict]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø°ÙˆØ°"""
        recommendations = []
        
        if anomaly_score >= 0.8:
            recommendations.append("ğŸš¨ ØªÙ†ÙÙŠØ° Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø·ÙˆØ§Ø±Ø¦ Ø§Ù„ÙÙˆØ±ÙŠØ©")
            recommendations.append("ğŸ”´ ØªÙØ¹ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¢Ù„ÙŠ")
            recommendations.append("ğŸ“ Ø¥Ø®Ø·Ø§Ø± ÙØ±ÙŠÙ‚ Ø§Ù„Ø·ÙˆØ§Ø±Ø¦")
        
        if anomaly_score >= 0.6:
            recommendations.append("âš ï¸ Ø²ÙŠØ§Ø¯Ø© ÙˆØªÙŠØ±Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¥Ù„Ù‰ ÙƒÙ„ 5 Ø«ÙˆØ§Ù†ÙŠ")
            recommendations.append("ğŸ”§ ÙØ­Øµ Ø§Ù„Ù…Ø¹Ø¯Ø§Øª Ø§Ù„Ø­Ø±Ø¬Ø©")
        
        for anomaly in critical_anomalies:
            if anomaly['severity'] == 'CRITICAL':
                recommendations.append(f"ğŸ”¥ ÙØ­Øµ Ø¹Ø§Ø¬Ù„ Ù„Ù…Ø³ØªØ´Ø¹Ø± {anomaly['sensor']}")
            elif anomaly['severity'] == 'WARNING':
                recommendations.append(f"ğŸ”¶ Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ø³ØªØ´Ø¹Ø± {anomaly['sensor']}")
        
        if not recommendations:
            recommendations.append("âœ… Ø§Ù„ÙˆØ¶Ø¹ Ø·Ø¨ÙŠØ¹ÙŠ - Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø±ÙˆØªÙŠÙ†ÙŠØ©")
        
        return recommendations
    
    def _update_adaptive_threshold(self, current_score: float):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„ØªÙƒÙŠÙÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ø¯ÙŠØ«"""
        try:
            if len(self.anomaly_history) > 50:
                recent_scores = [entry['anomaly_score'] for entry in self.anomaly_history[-50:]]
                current_percentile = np.percentile(recent_scores, 95)
                
                # ØªØ­Ø¯ÙŠØ« Ø³Ù„Ø³ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
                self.adaptive_threshold = 0.95 * self.adaptive_threshold + 0.05 * current_percentile
                
        except Exception as e:
            self.logger.error(f"âŒ Adaptive threshold update failed: {e}")
    
    def _update_anomaly_history(self, result: Dict[str, Any]):
        """ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ø´Ø°ÙˆØ° Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        self.anomaly_history.append(result)
        
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ 1000 ØªØ³Ø¬ÙŠÙ„ ÙÙ‚Ø· Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
        if len(self.anomaly_history) > 1000:
            self.anomaly_history = self.anomaly_history[-1000:]
    
    def _get_untrained_response(self) -> Dict[str, Any]:
        """Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø±Ø¨Ø©"""
        return {
            'is_anomaly': False,
            'anomaly_score': 0.0,
            'risk_level': 'LOW',
            'ensemble_results': {},
            'autoencoder_result': {},
            'critical_anomalies': [],
            'temporal_analysis': {},
            'data_quality_issues': ['Models not trained'],
            'confidence': 0.1,
            'adaptive_threshold': 0.05,
            'recommendations': ['System initializing - training required'],
            'timestamp': datetime.now(),
            'warning': 'Models not trained'
        }
    
    def get_system_status(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        return {
            'is_trained': self.is_trained,
            'adaptive_threshold': float(self.adaptive_threshold),
            'anomaly_history_count': len(self.anomaly_history),
            'performance_metrics': self.performance_metrics,
            'data_quality_checks': True,
            'temporal_analysis_enabled': True,
            'ensemble_models_active': True,
            'last_training': datetime.now() if self.is_trained else None,
            'system_confidence': 0.95 if self.is_trained else 0.1
        }
        class AdvancedPredictionEngine:
    """Ù…Ø­Ø±Ùƒ ØªÙ†Ø¨Ø¤ Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© - SS Rating"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('SmartNeural.AI.Prediction')
        
        # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self.lstm_model = self._build_advanced_lstm()
        self.transformer_model = self._build_transformer_model()
        self.hybrid_model = self._build_hybrid_model()
        
        # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        self.active_model = None
        self.model_performance = {}
        self.sequence_length = config['prediction']['sequence_length']
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self.scalers = {
            'features': StandardScaler(),
            'target': StandardScaler()
        }
        
        self.is_trained = False
        self.prediction_history = []
        self.confidence_calibrator = ConfidenceCalibrator()
        
        self.logger.info("âœ… Advanced Prediction Engine Initialized - SS Rating")
    
    def _build_advanced_lstm(self) -> nn.Module:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ LSTM Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªÙ†Ø¨Ø¤"""
        class AdvancedLSTM(nn.Module):
            def __init__(self, input_size=6, hidden_size=128, num_layers=3, output_size=6, dropout=0.3):
                super().__init__()
                self.hidden_size = hidden_size
                self.num_layers = num_layers
                
                # LSTM Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ù…Ø¹ ØªØ³Ø±Ø¨
                self.lstm = nn.LSTM(
                    input_size, hidden_size, num_layers,
                    batch_first=True, dropout=dropout, bidirectional=True
                )
                
                # ØªÙˆØ¬Ù‡ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
                self.attention = nn.MultiheadAttention(
                    embed_dim=hidden_size * 2,  # Ù„Ø£Ù†Ù‡Ø§ bidirectional
                    num_heads=8,
                    dropout=0.1,
                    batch_first=True
                )
                
                # Ø·Ø¨Ù‚Ø§Øª ÙƒØ«ÙŠÙØ© Ù…ØªÙ‚Ø¯Ù…Ø©
                self.fc_layers = nn.Sequential(
                    nn.Linear(hidden_size * 2, 256),
                    nn.LeakyReLU(0.1),
                    nn.BatchNorm1d(256),
                    nn.Dropout(0.2),
                    nn.Linear(256, 128),
                    nn.LeakyReLU(0.1),
                    nn.BatchNorm1d(128),
                    nn.Dropout(0.2),
                    nn.Linear(128, output_size),
                    nn.Tanh()
                )
                
                # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
                self._init_weights()
            
            def _init_weights(self):
                """ØªÙ‡ÙŠØ¦Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø£ÙˆØ²Ø§Ù†"""
                for name, param in self.named_parameters():
                    if 'weight' in name:
                        if 'lstm' in name:
                            nn.init.orthogonal_(param)
                        elif 'attention' in name:
                            nn.init.xavier_uniform_(param)
                        elif 'fc' in name:
                            nn.init.kaiming_normal_(param, nonlinearity='leaky_relu')
            
            def forward(self, x):
                # LSTM
                lstm_out, (hidden, cell) = self.lstm(x)
                
                # Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
                attended_out, attention_weights = self.attention(
                    lstm_out, lstm_out, lstm_out
                )
                
                # Ø£Ø®Ø° Ø¢Ø®Ø± Ø®Ø·ÙˆØ© Ø²Ù…Ù†ÙŠØ©
                last_output = attended_out[:, -1, :]
                
                # Ø·Ø¨Ù‚Ø§Øª ÙƒØ«ÙŠÙØ©
                output = self.fc_layers(last_output)
                return output, attention_weights
        
        return AdvancedLSTM(
            input_size=6,
            hidden_size=128,
            num_layers=3,
            output_size=6,
            dropout=0.3
        )
    
    def _build_transformer_model(self) -> nn.Module:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Transformer Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø²Ù…Ù†ÙŠ"""
        class TimeSeriesTransformer(nn.Module):
            def __init__(self, input_dim=6, model_dim=128, num_heads=8, num_layers=4, output_dim=6):
                super().__init__()
                self.model_dim = model_dim
                
                # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…ÙˆØ¶Ø¹
                self.positional_encoding = PositionalEncoding(model_dim)
                
                # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
                self.input_projection = nn.Linear(input_dim, model_dim)
                
                # Ø·Ø¨Ù‚Ø§Øª Transformer
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=model_dim,
                    nhead=num_heads,
                    dim_feedforward=512,
                    dropout=0.1,
                    activation='gelu',
                    batch_first=True
                )
                self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
                
                # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
                self.output_layer = nn.Sequential(
                    nn.Linear(model_dim, 256),
                    nn.GELU(),
                    nn.Dropout(0.2),
                    nn.Linear(256, 128),
                    nn.GELU(),
                    nn.Dropout(0.1),
                    nn.Linear(128, output_dim),
                    nn.Tanh()
                )
            
            def forward(self, x):
                # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
                x = self.input_projection(x)
                
                # Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹ÙŠ
                x = self.positional_encoding(x)
                
                # Transformer
                transformer_out = self.transformer(x)
                
                # Ø£Ø®Ø° Ø¢Ø®Ø± Ø®Ø·ÙˆØ© Ø²Ù…Ù†ÙŠØ©
                last_timestep = transformer_out[:, -1, :]
                
                # Ø¥Ø®Ø±Ø§Ø¬
                output = self.output_layer(last_timestep)
                return output
        
        return TimeSeriesTransformer()
    
    def _build_hybrid_model(self) -> nn.Module:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù‡Ø¬ÙŠÙ† LSTM-Transformer"""
        class HybridModel(nn.Module):
            def __init__(self, input_dim=6, lstm_hidden=128, transformer_dim=128, output_dim=6):
                super().__init__()
                
                # LSTM Branch
                self.lstm_branch = nn.LSTM(
                    input_dim, lstm_hidden, 2,
                    batch_first=True, bidirectional=True, dropout=0.2
                )
                
                # Transformer Branch
                self.transformer_branch = nn.TransformerEncoder(
                    nn.TransformerEncoderLayer(
                        d_model=input_dim,
                        nhead=8,
                        dim_feedforward=256,
                        dropout=0.1,
                        batch_first=True
                    ),
                    num_layers=2
                )
                
                # Ø§Ù†Ø¯Ù…Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª
                self.feature_fusion = nn.Sequential(
                    nn.Linear(lstm_hidden * 2 + input_dim, 256),  # LSTM bidirectional + Transformer
                    nn.ReLU(),
                    nn.BatchNorm1d(256),
                    nn.Dropout(0.3),
                    nn.Linear(256, 128),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(128, output_dim),
                    nn.Tanh()
                )
            
            def forward(self, x):
                # LSTM
                lstm_out, _ = self.lstm_branch(x)
                lstm_features = lstm_out[:, -1, :]
                
                # Transformer
                transformer_out = self.transformer_branch(x)
                transformer_features = transformer_out[:, -1, :]
                
                # Ø§Ù†Ø¯Ù…Ø§Ø¬
                combined = torch.cat([lstm_features, transformer_features], dim=1)
                output = self.feature_fusion(combined)
                return output
        
        return HybridModel()
    
    def prepare_sequences(self, sensor_data: List[Dict[str, Any]]) -> Tuple[np.ndarray, np.ndarray]:
        """ØªØ­Ø¶ÙŠØ± Ù…ØªÙˆØ§Ù„ÙŠØ§Øª Ø²Ù…Ù†ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            features = []
            for data in sensor_data:
                feature_vector = [
                    data.get('pressure', 0.0),
                    data.get('temperature', 0.0),
                    data.get('methane', 0.0),
                    data.get('hydrogen_sulfide', 0.0),
                    data.get('vibration', 0.0),
                    data.get('flow', 0.0)
                ]
                features.append(feature_vector)
            
            features_array = np.array(features)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ØªÙˆØ§Ù„ÙŠØ§Øª
            X, y = [], []
            for i in range(len(features_array) - self.sequence_length):
                X.append(features_array[i:i + self.sequence_length])
                y.append(features_array[i + self.sequence_length])
            
            return np.array(X), np.array(y)
            
        except Exception as e:
            self.logger.error(f"âŒ Sequence preparation failed: {e}")
            return np.array([]), np.array([])
    
    def train_models(self, training_data: List[Dict[str, Any]], validation_data: List[Dict[str, Any]] = None):
        """ØªØ¯Ø±ÙŠØ¨ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            self.logger.info("ğŸ”„ Training advanced prediction models...")
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train, y_train = self.prepare_sequences(training_data)
            
            if len(X_train) == 0:
                self.logger.error("âŒ No training sequences generated")
                return
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³ÙƒØ§Ù„Ø±
            X_reshaped = X_train.reshape(-1, X_train.shape[-1])
            self.scalers['features'].fit(X_reshaped)
            self.scalers['target'].fit(y_train)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_normalized = self.scalers['features'].transform(X_reshaped).reshape(X_train.shape)
            y_normalized = self.scalers['target'].transform(y_train)
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Tensor
            X_tensor = torch.FloatTensor(X_normalized)
            y_tensor = torch.FloatTensor(y_normalized)
            
            dataset = TensorDataset(X_tensor, y_tensor)
            dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            models = {
                'lstm': self.lstm_model,
                'transformer': self.transformer_model,
                'hybrid': self.hybrid_model
            }
            
            for name, model in models.items():
                self._train_single_model(model, name, dataloader, validation_data)
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
            self._select_best_model()
            self.is_trained = True
            
            self.logger.info("âœ… All prediction models trained successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ Prediction models training failed: {e}")
    
    def _train_single_model(self, model: nn.Module, model_name: str, 
                          dataloader: DataLoader, validation_data: List[Dict[str, Any]] = None):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ÙØ±Ø¯ÙŠ Ù…ØªÙ‚Ø¯Ù…"""
        try:
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            criterion = nn.MSELoss()
            optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)
            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)
            
            training_losses = []
            validation_losses = []
            
            # Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            for epoch in range(100):  # epochs
                model.train()
                epoch_loss = 0.0
                
                for batch_X, batch_y in dataloader:
                    optimizer.zero_grad()
                    
                    if model_name == 'lstm':
                        predictions, _ = model(batch_X)
                    else:
                        predictions = model(batch_X)
                    
                    loss = criterion(predictions, batch_y)
                    loss.backward()
                    
                    # Ù‚Øµ Ø§Ù„ØªØ¯Ø±Ø¬
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                
                scheduler.step()
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©
                avg_loss = epoch_loss / len(dataloader)
                training_losses.append(avg_loss)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ§Ø­Ø©
                if validation_data and epoch % 10 == 0:
                    val_loss = self._validate_model(model, model_name, validation_data)
                    validation_losses.append(val_loss)
                    self.logger.info(f"ğŸ“Š {model_name} Epoch {epoch}, Train Loss: {avg_loss:.6f}, Val Loss: {val_loss:.6f}")
                else:
                    if epoch % 25 == 0:
                        self.logger.info(f"ğŸ“Š {model_name} Epoch {epoch}, Loss: {avg_loss:.6f}")
            
            # Ø­ÙØ¸ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self.model_performance[model_name] = {
                'training_loss': training_losses,
                'validation_loss': validation_losses,
                'final_train_loss': training_losses[-1] if training_losses else float('inf'),
                'final_val_loss': validation_losses[-1] if validation_losses else float('inf')
            }
            
        except Exception as e:
            self.logger.error(f"âŒ {model_name} training failed: {e}")
    
    def _validate_model(self, model: nn.Module, model_name: str, validation_data: List[Dict[str, Any]]) -> float:
        """ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            model.eval()
            criterion = nn.MSELoss()
            
            X_val, y_val = self.prepare_sequences(validation_data)
            if len(X_val) == 0:
                return float('inf')
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_reshaped = X_val.reshape(-1, X_val.shape[-1])
            X_normalized = self.scalers['features'].transform(X_reshaped).reshape(X_val.shape)
            y_normalized = self.scalers['target'].transform(y_val)
            
            X_tensor = torch.FloatTensor(X_normalized)
            y_tensor = torch.FloatTensor(y_normalized)
            
            with torch.no_grad():
                if model_name == 'lstm':
                    predictions, _ = model(X_tensor)
                else:
                    predictions = model(X_tensor)
                
                loss = criterion(predictions, y_tensor)
            
            return loss.item()
            
        except Exception as e:
            self.logger.error(f"âŒ Model validation failed: {e}")
            return float('inf')
    
    def _select_best_model(self):
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            best_model_name = None
            best_loss = float('inf')
            
            for name, performance in self.model_performance.items():
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ù† Ø£Ù…ÙƒÙ†ØŒ ÙˆØ¥Ù„Ø§ Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
                loss = performance.get('final_val_loss', performance.get('final_train_loss', float('inf')))
                
                if loss < best_loss:
                    best_loss = loss
                    best_model_name = name
            
            if best_model_name == 'lstm':
                self.active_model = self.lstm_model
            elif best_model_name == 'transformer':
                self.active_model = self.transformer_model
            else:
                self.active_model = self.hybrid_model
            
            self.logger.info(f"ğŸ¯ Selected best model: {best_model_name} with loss: {best_loss:.6f}")
            
        except Exception as e:
            self.logger.error(f"âŒ Model selection failed: {e}")
            self.active_model = self.lstm_model  # Ù†Ù…ÙˆØ°Ø¬ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¢Ù…Ù†
    
    def predict(self, sensor_data: List[Dict[str, Any]], steps: int = 1) -> Dict[str, Any]:
        """ØªÙ†Ø¨Ø¤ Ù…ØªÙ‚Ø¯Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª"""
        try:
            if not self.is_trained or self.active_model is None:
                return self._get_untrained_prediction()
            
            # ØªØ­Ø¶ÙŠØ± Ø£Ø­Ø¯Ø« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            recent_data = sensor_data[-self.sequence_length:]
            if len(recent_data) < self.sequence_length:
                self.logger.warning("âš ï¸ Insufficient data for prediction")
                return self._get_insufficient_data_prediction()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªÙˆØ§Ù„ÙŠØ© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            input_sequence = self._create_input_sequence(recent_data)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…ØªÙƒØ±Ø±
            predictions = []
            confidence_scores = []
            current_sequence = input_sequence.clone()
            
            for step in range(steps):
                with torch.no_grad():
                    if isinstance(self.active_model, self.lstm_model.__class__):
                        pred, attention_weights = self.active_model(current_sequence)
                    else:
                        pred = self.active_model(current_sequence)
                        attention_weights = None
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø©
                confidence = self.confidence_calibrator.calculate_confidence(
                    pred, current_sequence, step
                )
                
                predictions.append(pred.numpy())
                confidence_scores.append(confidence)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ØªÙˆØ§Ù„ÙŠØ© Ù„Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©
                if step < steps - 1:
                    current_sequence = self._update_sequence(current_sequence, pred)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            predictions_array = np.array(predictions).squeeze()
            final_predictions = self.scalers['target'].inverse_transform(predictions_array)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            risk_assessment = self._assess_prediction_risk(final_predictions, confidence_scores)
            trends = self._analyze_prediction_trends(final_predictions)
            
            result = {
                'predictions': final_predictions.tolist(),
                'confidence_scores': confidence_scores,
                'average_confidence': np.mean(confidence_scores),
                'risk_level': risk_assessment['risk_level'],
                'risk_factors': risk_assessment['risk_factors'],
                'trends': trends,
                'model_used': self.active_model.__class__.__name__,
                'timestamp': datetime.now(),
                'steps_ahead': steps,
                'recommendations': self._generate_prediction_recommendations(
                    final_predictions, risk_assessment, trends
                )
            }
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„
            self._update_prediction_history(result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Advanced prediction failed: {e}")
            return {
                'error': str(e),
                'predictions': [],
                'confidence_scores': [],
                'risk_level': 'UNKNOWN',
                'recommendations': ['Prediction system error - check logs']
            }
    
    def _create_input_sequence(self, sensor_data: List[Dict[str, Any]]) -> torch.Tensor:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªÙˆØ§Ù„ÙŠØ© Ø¥Ø¯Ø®Ø§Ù„ Ù„Ù„ØªÙ†Ø¨Ø¤"""
        features = []
        for data in sensor_data:
            feature_vector = [
                data.get('pressure', 0.0),
                data.get('temperature', 0.0),
                data.get('methane', 0.0),
                data.get('hydrogen_sulfide', 0.0),
                data.get('vibration', 0.0),
                data.get('flow', 0.0)
            ]
            features.append(feature_vector)
        
        features_array = np.array(features)
        normalized_features = self.scalers['features'].transform(features_array)
        
        return torch.FloatTensor(normalized_features).unsqueeze(0)
    
    def _update_sequence(self, current_sequence: torch.Tensor, new_prediction: torch.Tensor) -> torch.Tensor:
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ØªÙˆØ§Ù„ÙŠØ© Ø¨Ø¥Ø¶Ø§ÙØ© ØªÙ†Ø¨Ø¤ Ø¬Ø¯ÙŠØ¯"""
        # Ø¥Ø²Ø§Ù„Ø© Ø£Ù‚Ø¯Ù… Ù†Ù‚Ø·Ø© ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¬Ø¯ÙŠØ¯
        updated_sequence = torch.cat([
            current_sequence[:, 1:, :],  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£ÙˆÙ„
            new_prediction.unsqueeze(1)  # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯
        ], dim=1)
        
        return updated_sequence
    
    def _assess_prediction_risk(self, predictions: np.ndarray, confidence_scores: List[float]) -> Dict[str, Any]:
        """ØªÙ‚ÙŠÙŠÙ… Ù…Ø®Ø§Ø·Ø± Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        risk_factors = []
        
        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ø³ØªØ´Ø¹Ø±
        sensor_limits = {
            'pressure': (0, 200),
            'temperature': (0, 300),
            'methane': (0, 5000),
            'hydrogen_sulfide': (0, 500),
            'vibration': (0, 20),
            'flow': (0, 500)
        }
        
        for i, (sensor, (min_val, max_val)) in enumerate(sensor_limits.items()):
            sensor_predictions = predictions[:, i] if predictions.ndim > 1 else predictions[i]
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ÙˆØ¯
            if np.any(sensor_predictions > max_val * 0.9):  # 90% Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
                risk_factors.append(f"{sensor} approaching upper limit")
            
            if np.any(sensor_predictions < min_val * 1.1):  # Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰
                risk_factors.append(f"{sensor} near lower limit")
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù„Ø¨
            if len(sensor_predictions) > 1:
                volatility = np.std(sensor_predictions)
                if volatility > (max_val - min_val) * 0.1:  # ØªÙ‚Ù„Ø¨ Ø¹Ø§Ù„ÙŠ
                    risk_factors.append(f"High volatility in {sensor}")
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø«Ù‚Ø©
        low_confidence_count = sum(1 for conf in confidence_scores if conf < 0.6)
        if low_confidence_count > len(confidence_scores) * 0.5:
            risk_factors.append("Multiple low-confidence predictions")
        
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©
        if len(risk_factors) >= 3:
            risk_level = "HIGH"
        elif len(risk_factors) >= 1:
            risk_level = "MEDIUM"
        else:
            risk_level = "LOW"
        
        return {
            'risk_level': risk_level,
            'risk_factors': risk_factors,
            'confidence_issues': low_confidence_count
        }
    
    def _analyze_prediction_trends(self, predictions: np.ndarray) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        if predictions.ndim == 1 or len(predictions) < 2:
            return {'trend': 'stable', 'momentum': 0.0}
        
        trends = []
        momentums = []
        
        for i in range(predictions.shape[1] if predictions.ndim > 1 else 1):
            if predictions.ndim > 1:
                series = predictions[:, i]
            else:
                series = predictions
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            if len(series) >= 2:
                x = np.arange(len(series))
                slope, _, _, _, _ = stats.linregress(x, series)
                
                trends.append('increasing' if slope > 0.01 else 'decreasing' if slope < -0.01 else 'stable')
                momentums.append(abs(slope))
        
        return {
            'trends': trends,
            'dominant_trend': max(set(trends), key=trends.count) if trends else 'stable',
            'average_momentum': np.mean(momentums) if momentums else 0.0,
            'volatility': np.std(predictions) if predictions.size > 1 else 0.0
        }
    
    def _generate_prediction_recommendations(self, predictions: np.ndarray, 
                                           risk_assessment: Dict[str, Any], 
                                           trends: Dict[str, Any]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        recommendations = []
        
        risk_level = risk_assessment['risk_level']
        risk_factors = risk_assessment['risk_factors']
        
        if risk_level == "HIGH":
            recommendations.append("ğŸš¨ ØªÙ†Ø¨Ø¤ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø®Ø·ÙˆØ±Ø© - ØªÙØ¹ÙŠÙ„ Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„Ø§Øª Ø§Ù„Ø·ÙˆØ§Ø±Ø¦")
            recommendations.append("ğŸ“Š Ø²ÙŠØ§Ø¯Ø© ÙˆØªÙŠØ±Ø© Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ÙƒÙ„ Ø«Ø§Ù†ÙŠØ©")
            recommendations.append("ğŸ‘¥ Ø¥Ø®Ø·Ø§Ø± Ø§Ù„ÙÙ†ÙŠÙŠÙ† Ù„Ù„ØªØ¯Ø®Ù„ Ø§Ù„ÙÙˆØ±ÙŠ")
        
        elif risk_level == "MEDIUM":
            recommendations.append("âš ï¸ Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…ØªØ²Ø§ÙŠØ¯Ø© Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø®Ø·ÙˆØ±Ø©")
            recommendations.append("ğŸ”§ ÙØ­Øµ ÙˆÙ‚Ø§Ø¦ÙŠ Ù„Ù„Ù…Ø¹Ø¯Ø§Øª")
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø®Ø·ÙˆØ±Ø©
        for factor in risk_factors:
            if "pressure" in factor:
                recommendations.append("â›½ ÙØ­Øµ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¶ØºØ· ÙˆØ§Ù„Ù…Ø¶Ø®Ø§Øª")
            elif "temperature" in factor:
                recommendations.append("ğŸ”¥ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ¨Ø±ÙŠØ¯")
            elif "methane" in factor or "hydrogen_sulfide" in factor:
                recommendations.append("â˜ ï¸ ØªÙØ¹ÙŠÙ„ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªØ³Ø±Ø¨")
            elif "vibration" in factor:
                recommendations.append("ğŸ“³ ÙØ­Øµ Ø§Ù„ØªÙˆØ§Ø²Ù† ÙˆØ§Ù„Ù…Ø­Ø§Ù…Ù„ Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠØ©")
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
        if trends.get('dominant_trend') == 'increasing':
            recommendations.append("ğŸ“ˆ Ø§ØªØ¬Ø§Ù‡ ØªØµØ§Ø¹Ø¯ÙŠ - Ø§Ù„Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ù„ØªØ¯Ø§Ø¨ÙŠØ± Ø§Ø­ØªØ±Ø§Ø²ÙŠØ©")
        elif trends.get('dominant_trend') == 'decreasing':
            recommendations.append("ğŸ“‰ Ø§ØªØ¬Ø§Ù‡ ØªÙ†Ø§Ø²Ù„ÙŠ - Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±")
        
        if not recommendations:
            recommendations.append("âœ… Ø§Ù„ÙˆØ¶Ø¹ Ù…Ø³ØªÙ‚Ø± - Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø±ÙˆØªÙŠÙ†ÙŠØ©")
        
        return recommendations
    
    def _get_untrained_prediction(self) -> Dict[str, Any]:
        """Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø±Ø¨Ø©"""
        return {
            'predictions': [],
            'confidence_scores': [],
            'average_confidence': 0.1,
            'risk_level': 'UNKNOWN',
            'risk_factors': ['Prediction models not trained'],
            'trends': {},
            'model_used': 'None',
            'timestamp': datetime.now(),
            'steps_ahead': 0,
            'recommendations': ['System initializing - training required'],
            'warning': 'Models not trained'
        }
    
    def _get_insufficient_data_prediction(self) -> Dict[str, Any]:
        """Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©"""
        return {
            'predictions': [],
            'confidence_scores': [],
            'average_confidence': 0.0,
            'risk_level': 'LOW',
            'risk_factors': ['Insufficient historical data'],
            'trends': {},
            'model_used': 'None',
            'timestamp': datetime.now(),
            'steps_ahead': 0,
            'recommendations': ['Collect more data for accurate predictions'],
            'warning': 'Insufficient data'
        }
    
    def _update_prediction_history(self, result: Dict[str, Any]):
        """ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        self.prediction_history.append(result)
        
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ 500 ØªÙ†Ø¨Ø¤ Ø­Ø¯ÙŠØ« ÙÙ‚Ø·
        if len(self.prediction_history) > 500:
            self.prediction_history = self.prediction_history[-500:]

class PositionalEncoding(nn.Module):
    """ØªØ±Ù…ÙŠØ² Ù…ÙˆØ¶Ø¹ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØªØ§Ø¨Ø¹ÙŠØ©"""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹ÙŠ
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:, :x.size(1)]

class ConfidenceCalibrator:
    """Ù…Ø¹Ø§ÙŠØ± Ø«Ù‚Ø© Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
    
    def __init__(self):
        self.prediction_errors = []
        self.confidence_history = []
    
    def calculate_confidence(self, prediction: torch.Tensor, 
                           input_sequence: torch.Tensor, 
                           step: int) -> float:
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªÙ†Ø¨Ø¤"""
        try:
            # Ø«Ù‚Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§ØªØ³Ø§Ù‚ Ø§Ù„ØªÙ†Ø¨Ø¤
            consistency_score = self._calculate_consistency(prediction, input_sequence)
            
            # Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø®Ø·ÙˆØ© Ø§Ù„ØªÙ†Ø¨Ø¤ (ØªÙ‚Ù„ ÙƒÙ„Ù…Ø§ Ø§Ø¨ØªØ¹Ø¯Ù†Ø§)
            step_penalty = 1.0 / (1.0 + step * 0.1)
            
            # Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ…
            distribution_score = self._calculate_distribution_confidence(prediction)
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            final_confidence = (consistency_score * 0.4 + 
                              step_penalty * 0.3 + 
                              distribution_score * 0.3)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„
            self.confidence_history.append(final_confidence)
            
            return max(0.1, min(1.0, final_confidence))
            
        except Exception as e:
            logging.getLogger('SmartNeural.AI.Confidence').error(f"Confidence calculation failed: {e}")
            return 0.5  # Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
    
    def _calculate_consistency(self, prediction: torch.Tensor, input_sequence: torch.Tensor) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§ØªØ³Ø§Ù‚ Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©"""
        try:
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
            historical_mean = torch.mean(input_sequence, dim=1)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù† Ø§Ù„Ù…ØªÙˆØ³Ø·
            deviation = torch.abs(prediction - historical_mean)
            normalized_deviation = torch.mean(deviation / (torch.std(input_sequence, dim=1) + 1e-8))
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø¥Ù„Ù‰ Ø«Ù‚Ø© (ÙƒÙ„Ù…Ø§ Ù‚Ù„ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø²Ø§Ø¯Øª Ø§Ù„Ø«Ù‚Ø©)
            consistency = 1.0 / (1.0 + normalized_deviation.item())
            
            return consistency
            
        except Exception:
            return 0.5
    
    def _calculate_distribution_confidence(self, prediction: torch.Tensor) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹"""
        try:
            # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù‚ØµÙˆÙ‰ Ø£Ù‚Ù„ Ø«Ù‚Ø©
            max_values = torch.tensor([200, 300, 5000, 500, 20, 500])  # Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ´Ø¹Ø±Ø§Øª
            normalized_pred = torch.abs(prediction) / max_values
            
            # Ø§Ù„Ø«Ù‚Ø© ØªÙ‚Ù„ Ø¹Ù†Ø¯Ù…Ø§ ØªÙ‚ØªØ±Ø¨ Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯
            boundary_distance = 1.0 - torch.max(normalized_pred)
            
            return max(0.0, boundary_distance.item())
            
        except Exception:
            return 0.5

class AdaptiveLearningSystem:
    """Ù†Ø¸Ø§Ù… ØªØ¹Ù„Ù… ØªÙƒÙŠÙÙŠ Ù…ØªÙ‚Ø¯Ù… - SS Rating"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('SmartNeural.AI.AdaptiveLearning')
        
        # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.performance_metrics = {}
        self.model_drift_detector = ModelDriftDetector()
        self.concept_drift_tracker = ConceptDriftTracker()
        
        # Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø´Ø·
        self.uncertainty_sampler = UncertaintySampler()
        self.feedback_loop = FeedbackLoop()
        
        # Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
        self.hyperparameter_optimizer = HyperparameterOptimizer()
        
        self.learning_rate = 0.001
        self.retraining_interval = timedelta(hours=24)
        self.last_retraining = None
        
        self.logger.info("âœ… Adaptive Learning System Initialized - SS Rating")
    
    def monitor_model_performance(self, predictions: Dict[str, Any], 
                                actual_values: Dict[str, Any]) -> Dict[str, Any]:
        """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ…Ø±"""
        try:
            performance_report = {
                'timestamp': datetime.now(),
                'prediction_accuracy': self._calculate_prediction_accuracy(predictions, actual_values),
                'anomaly_detection_accuracy': self._calculate_anomaly_accuracy(predictions, actual_values),
                'model_drift': self.model_drift_detector.detect_drift(predictions, actual_values),
                'concept_drift': self.concept_drift_tracker.track_changes(actual_values),
                'data_quality': self._assess_data_quality(actual_values),
                'recommendations': []
            }
            
            # ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
            if performance_report['model_drift']['detected']:
                performance_report['recommendations'].append("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø³Ø¨Ø¨ Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ø£Ø¯Ø§Ø¡")
            
            if performance_report['concept_drift']['detected']:
                performance_report['recommendations'].append("ğŸ¯ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø³Ø¨Ø¨ ØªØºÙŠØ± Ø§Ù„Ø£Ù†Ù…Ø§Ø·")
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
            self._update_performance_metrics(performance_report)
            
            return performance_report
            
        except Exception as e:
            self.logger.error(f"âŒ Performance monitoring failed: {e}")
            return {'error': str(e)}
    
    def _calculate_prediction_accuracy(self, predictions: Dict[str, Any], 
                                    actual_values: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            if 'predictions' not in predictions or not actual_values:
                return 0.0
            
            pred_array = np.array(predictions['predictions'])
            actual_array = np.array([
                actual_values.get('pressure', 0.0),
                actual_values.get('temperature', 0.0),
                actual_values.get('methane', 0.0),
                actual_values.get('hydrogen_sulfide', 0.0),
                actual_values.get('vibration', 0.0),
                actual_values.get('flow', 0.0)
            ])
            
            # Ø­Ø³Ø§Ø¨ Ø®Ø·Ø£ Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ø§Ù„Ù…ØªÙˆØ³Ø·
            if pred_array.ndim > 1 and pred_array.shape[0] > 0:
                mse = np.mean((pred_array[0] - actual_array) ** 2)
                accuracy = 1.0 / (1.0 + np.sqrt(mse))
                return min(1.0, accuracy)
            
            return 0.5  # Ø¯Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            
        except Exception:
            return 0.5
    
    def _calculate_anomaly_accuracy(self, predictions: Dict[str, Any], 
                                  actual_values: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° (Ù…Ø­Ø§ÙƒØ§Ø©)"""
        # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ØªØ­ØªØ§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚Ø© Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ°
        return 0.85  # Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ø¯Ù‚Ø© Ø¬ÙŠØ¯Ø©
    
    def _assess_data_quality(self, sensor_data: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        quality_metrics = {
            'completeness': 1.0,  # Ù†Ø³Ø¨Ø© Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            'consistency': 1.0,   # Ø§ØªØ³Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            'timeliness': 1.0,    # Ø­Ø¯Ø§Ø«Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            'validity': 1.0       # ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        for sensor, value in sensor_data.items():
            if value is None:
                quality_metrics['completeness'] -= 0.1
            elif not isinstance(value, (int, float)):
                quality_metrics['validity'] -= 0.1
        
        return quality_metrics
    
    def _update_performance_metrics(self, performance_report: Dict[str, Any]):
        """ØªØ­Ø¯ÙŠØ« Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©"""
        timestamp = performance_report['timestamp']
        
        for metric, value in performance_report.items():
            if metric not in ['timestamp', 'recommendations']:
                if metric not in self.performance_metrics:
                    self.performance_metrics[metric] = []
                
                self.performance_metrics[metric].append({
                    'timestamp': timestamp,
                    'value': value
                })
                
                # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ 1000 Ù‚ÙŠØ§Ø³ Ø­Ø¯ÙŠØ« ÙÙ‚Ø·
                if len(self.performance_metrics[metric]) > 1000:
                    self.performance_metrics[metric] = self.performance_metrics[metric][-1000:]

class ModelDriftDetector:
    """ÙƒØ§Ø´Ù Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    
    def __init__(self):
        self.prediction_errors = []
        self.drift_threshold = 0.1
    
    def detect_drift(self, predictions: Dict[str, Any], actual_values: Dict[str, Any]) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            # Ø­Ø³Ø§Ø¨ Ø®Ø·Ø£ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø­Ø§Ù„ÙŠ
            current_error = self._calculate_prediction_error(predictions, actual_values)
            self.prediction_errors.append(current_error)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©
            if len(self.prediction_errors) > 50:
                recent_errors = self.prediction_errors[-50:]
                historical_errors = self.prediction_errors[-100:-50] if len(self.prediction_errors) > 100 else recent_errors
                
                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± t
                t_stat, p_value = stats.ttest_ind(recent_errors, historical_errors)
                
                drift_detected = p_value < 0.05 and np.mean(recent_errors) > np.mean(historical_errors)
                
                return {
                    'detected': drift_detected,
                    'p_value': p_value,
                    'current_error': current_error,
                    'error_increase': np.mean(recent_errors) - np.mean(historical_errors) if drift_detected else 0.0
                }
            else:
                return {
                    'detected': False,
                    'p_value': 1.0,
                    'current_error': current_error,
                    'error_increase': 0.0
                }
                
        except Exception as e:
            logging.getLogger('SmartNeural.AI.DriftDetection').error(f"Drift detection failed: {e}")
            return {'detected': False, 'error': str(e)}
    
    def _calculate_prediction_error(self, predictions: Dict[str, Any], actual_values: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø®Ø·Ø£ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            if 'predictions' not in predictions or not actual_values:
                return 1.0  # Ø£Ù‚ØµÙ‰ Ø®Ø·Ø£
            
            pred_values = predictions['predictions']
            if not pred_values or len(pred_values) == 0:
                return 1.0
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆÙ„ ØªÙ†Ø¨Ø¤ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
            pred_array = np.array(pred_values[0] if isinstance(pred_values[0], list) else pred_values)
            actual_array = np.array([
                actual_values.get('pressure', 0.0),
                actual_values.get('temperature', 0.0),
                actual_values.get('methane', 0.0),
                actual_values.get('hydrogen_sulfide', 0.0),
                actual_values.get('vibration', 0.0),
                actual_values.get('flow', 0.0)
            ])
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ø§Ù„Ù…ØªÙˆØ³Ø·
            mse = np.mean((pred_array - actual_array) ** 2)
            return mse
            
        except Exception:
            return 1.0

class ConceptDriftTracker:
    """Ù…ØªØªØ¨Ø¹ Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…"""
    
    def __init__(self):
        self.data_distributions = []
        self.distribution_changes = []
    
    def track_changes(self, sensor_data: Dict[str, Any]) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ ØªØºÙŠØ±Ø§Øª ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ´Ø¹Ø±Ø§Øª
            values = np.array([
                sensor_data.get('pressure', 0.0),
                sensor_data.get('temperature', 0.0),
                sensor_data.get('methane', 0.0),
                sensor_data.get('hydrogen_sulfide', 0.0),
                sensor_data.get('vibration', 0.0),
                sensor_data.get('flow', 0.0)
            ])
            
            self.data_distributions.append(values)
            
            # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ 100 ØªÙˆØ²ÙŠØ¹ Ø­Ø¯ÙŠØ« ÙÙ‚Ø·
            if len(self.data_distributions) > 100:
                self.data_distributions = self.data_distributions[-100:]
            
            # ÙƒØ´Ù Ø§Ù„ØªØºÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©
            if len(self.data_distributions) > 30:
                recent_data = np.array(self.data_distributions[-30:])
                historical_data = np.array(self.data_distributions[-60:-30]) if len(self.data_distributions) > 60 else recent_data
                
                # Ø­Ø³Ø§Ø¨ ØªØºÙŠØ± Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø³Ø§ÙØ© ÙˆØ§Ø³Ø±Ø³ØªÙŠÙ†
                drift_detected = self._detect_distribution_drift(recent_data, historical_data)
                
                return {
                    'detected': drift_detected,
                    'recent_mean': np.mean(recent_data, axis=0).tolist(),
                    'historical_mean': np.mean(historical_data, axis=0).tolist(),
                    'change_magnitude': np.linalg.norm(
                        np.mean(recent_data, axis=0) - np.mean(historical_data, axis=0)
                    )
                }
            else:
                return {
                    'detected': False,
                    'recent_mean': [],
                    'historical_mean': [],
                    'change_magnitude': 0.0
                }
                
        except Exception as e:
            logging.getLogger('SmartNeural.AI.ConceptDrift').error(f"Concept drift tracking failed: {e}")
            return {'detected': False, 'error': str(e)}
    
    def _detect_distribution_drift(self, recent_data: np.ndarray, historical_data: np.ndarray) -> bool:
        """ÙƒØ´Ù Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ø­ØµØ§Ø¦ÙŠ"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙˆÙ„Ù…ÙˆØ¬ÙˆØ±ÙˆÙ-Ø³Ù…ÙŠØ±Ù†ÙˆÙ Ù„ÙƒÙ„ Ù…ÙŠØ²Ø©
            drift_detected = False
            
            for i in range(recent_data.shape[1]):
                ks_stat, p_value = stats.ks_2samp(historical_data[:, i], recent_data[:, i])
                if p_value < 0.01:  # Ø¹ØªØ¨Ø© ØµØ§Ø±Ù…Ø©
                    drift_detected = True
                    break
            
            return drift_detected
            
        except Exception:
            return False

class UncertaintySampler:
    """Ø¹ÙŠÙ†Ø© Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ† Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø´Ø·"""
    
    def __init__(self):
        self.uncertainty_threshold = 0.3
        self.sampling_history = []
    
    def should_sample(self, prediction_confidence: float, anomaly_score: float) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù„Ù„ØªØ¹Ù„Ù…"""
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø£Ùˆ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø´Ø°ÙˆØ° Ø¹Ø§Ù„ÙŠØ©
        return (prediction_confidence < self.uncertainty_threshold or 
                anomaly_score > 0.7)

class FeedbackLoop:
    """Ø­Ù„Ù‚Ø© ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ"""
    
    def __init__(self):
        self.feedback_data = []
        self.learning_rate = 0.01
    
    def add_feedback(self, prediction: Dict[str, Any], actual_value: Dict[str, Any], 
                    user_feedback: Optional[Dict[str, Any]] = None):
        """Ø¥Ø¶Ø§ÙØ© ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„ØªØ¹Ù„Ù…"""
        feedback_entry = {
            'timestamp': datetime.now(),
            'prediction': prediction,
            'actual': actual_value,
            'user_feedback': user_feedback,
            'error': self._calculate_feedback_error(prediction, actual_value)
        }
        
        self.feedback_data.append(feedback_entry)
        
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ 1000 Ø¹ÙŠÙ†Ø© ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø©
        if len(self.feedback_data) > 1000:
            self.feedback_data = self.feedback_data[-1000:]
    
    def _calculate_feedback_error(self, prediction: Dict[str, Any], actual_value: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø®Ø·Ø£ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        try:
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·Ø£ Ø¨ÙŠÙ† Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©
            pred_values = prediction.get('predictions', [])
            if not pred_values:
                return 1.0
            
            pred_array = np.array(pred_values[0] if isinstance(pred_values[0], list) else pred_values)
            actual_array = np.array([
                actual_value.get('pressure', 0.0),
                actual_value.get('temperature', 0.0),
                actual_value.get('methane', 0.0),
                actual_value.get('hydrogen_sulfide', 0.0),
                actual_value.get('vibration', 0.0),
                actual_value.get('flow', 0.0)
            ])
            
            return np.mean((pred_array - actual_array) ** 2)
            
        except Exception:
            return 1.0

class HyperparameterOptimizer:
    """Ù…Ø­Ø³Ù† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    
    def __init__(self):
        self.optimization_history = []
        self.best_parameters = {}
    
    def optimize_parameters(self, model_performance: Dict[str, Any]) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            # ØªØ­Ø³ÙŠÙ† Ø¨Ø³ÙŠØ· Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª (ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Bayesian Optimization)
            suggestions = {}
            
            if model_performance.get('prediction_accuracy', 0) < 0.8:
                suggestions['learning_rate'] = 'Consider decreasing learning rate'
                suggestions['architecture'] = 'Consider adding more layers'
            
            if model_performance.get('model_drift', {}).get('detected', False):
                suggestions['retraining'] = 'Schedule immediate retraining'
                suggestions['regularization'] = 'Increase regularization'
            
            return {
                'suggestions': suggestions,
                'confidence_boost': 'Consider confidence calibration' if 
                model_performance.get('prediction_accuracy', 0) < 0.7 else 'Confidence adequate',
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            logging.getLogger('SmartNeural.AI.HyperparameterOpt').error(f"Hyperparameter optimization failed: {e}")
            return {'suggestions': {}, 'error': str(e)}

class AISystemManager:
    """Ù…Ø¯ÙŠØ± Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ - SS Rating"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('SmartNeural.AI.Manager')
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        self.anomaly_system = AdvancedAnomalySystem(config)
        self.prediction_engine = AdvancedPredictionEngine(config)
        self.adaptive_learning = AdaptiveLearningSystem(config)
        
        # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ù„Ø©
        self.system_status = 'initializing'
        self.performance_metrics = {}
        self.alert_history = []
        
        # Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
        self._initialize_systems()
        self.logger.info("âœ… AI System Manager Initialized - SS Rating")
    
    def _initialize_systems(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
            self.system_status = 'initialized'
            self.logger.info("ğŸ¯ All AI systems initialized successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ AI systems initialization failed: {e}")
            self.system_status = 'error'
    
    def process_sensor_data(self, sensor_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ´Ø¹Ø±Ø§Øª"""
        try:
            start_time = datetime.now()
            
            # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
            anomaly_result = self.anomaly_system.detect_anomalies(sensor_data)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            prediction_result = self.prediction_engine.predict([sensor_data], steps=3)
            
            # Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ
            learning_result = self.adaptive_learning.monitor_model_performance(
                prediction_result, sensor_data
            )
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            final_result = {
                'timestamp': datetime.now(),
                'processing_time': (datetime.now() - start_time).total_seconds(),
                'anomaly_detection': anomaly_result,
                'prediction': prediction_result,
                'adaptive_learning': learning_result,
                'system_status': self.system_status,
                'overall_risk_assessment': self._assess_overall_risk(anomaly_result, prediction_result),
                'action_recommendations': self._generate_actions(anomaly_result, prediction_result, learning_result)
            }
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„Ø§Øª
            self._update_system_metrics(final_result)
            
            # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
            self._manage_alerts(final_result)
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ AI processing failed: {e}")
            return {
                'error': str(e),
                'timestamp': datetime.now(),
                'system_status': 'error'
            }
    
    def _assess_overall_risk(self, anomaly_result: Dict[str, Any], 
                           prediction_result: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
        try:
            anomaly_risk = anomaly_result.get('risk_level', 'LOW')
            prediction_risk = prediction_result.get('risk_level', 'LOW')
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø¥Ù„Ù‰ Ù‚ÙŠÙ… Ø±Ù‚Ù…ÙŠØ©
            risk_values = {
                'CRITICAL': 4, 'HIGH': 3, 'MEDIUM': 2, 'LOW': 1, 'UNKNOWN': 1
            }
            
            anomaly_score = risk_values.get(anomaly_risk, 1)
            prediction_score = risk_values.get(prediction_risk, 1)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©
            overall_score = max(anomaly_score, prediction_score)
            
            risk_levels = {
                4: 'CRITICAL', 3: 'HIGH', 2: 'MEDIUM', 1: 'LOW'
            }
            
            return {
                'level': risk_levels.get(overall_score, 'LOW'),
                'score': overall_score,
                'factors': {
                    'anomaly_risk': anomaly_risk,
                    'prediction_risk': prediction_risk,
                    'anomaly_score': anomaly_result.get('anomaly_score', 0.0),
                    'prediction_confidence': prediction_result.get('average_confidence', 0.0)
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Overall risk assessment failed: {e}")
            return {'level': 'UNKNOWN', 'score': 0, 'factors': {}}
    
    def _generate_actions(self, anomaly_result: Dict[str, Any], 
                        prediction_result: Dict[str, Any],
                        learning_result: Dict[str, Any]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        actions = []
        
        # Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø°ÙˆØ°
        anomaly_actions = anomaly_result.get('recommendations', [])
        actions.extend(anomaly_actions)
        
        # Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤
        prediction_actions = prediction_result.get('recommendations', [])
        actions.extend(prediction_actions)
        
        # Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù…
        learning_actions = learning_result.get('recommendations', [])
        actions.extend(learning_actions)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        unique_actions = list(dict.fromkeys(actions))
        
        return unique_actions[:10]  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ 10 Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª
    
    def _update_system_metrics(self, processing_result: Dict[str, Any]):
        """ØªØ­Ø¯ÙŠØ« Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        timestamp = processing_result['timestamp']
        
        # ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        if 'processing_time_metrics' not in self.performance_metrics:
            self.performance_metrics['processing_time_metrics'] = []
        
        self.performance_metrics['processing_time_metrics'].append({
            'timestamp': timestamp,
            'processing_time': processing_result['processing_time']
        })
        
        # ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ø®Ø·ÙˆØ±Ø©
        risk_level = processing_result['overall_risk_assessment']['level']
        if 'risk_level_metrics' not in self.performance_metrics:
            self.performance_metrics['risk_level_metrics'] = []
        
        self.performance_metrics['risk_level_metrics'].append({
            'timestamp': timestamp,
            'risk_level': risk_level
        })
    
    def _manage_alerts(self, processing_result: Dict[str, Any]):
        """Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©"""
        try:
            risk_level = processing_result['overall_risk_assessment']['level']
            
            if risk_level in ['HIGH', 'CRITICAL']:
                alert = {
                    'timestamp': datetime.now(),
                    'level': risk_level,
                    'message': f"ØªÙ†Ø¨ÙŠÙ‡ {risk_level}: ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø´Ø°ÙˆØ° Ø­Ø±Ø¬ Ø£Ùˆ ØªÙ†Ø¨Ø¤ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø®Ø·ÙˆØ±Ø©",
                    'details': {
                        'anomaly_score': processing_result['anomaly_detection'].get('anomaly_score', 0),
                        'prediction_risk': processing_result['prediction'].get('risk_level', 'UNKNOWN'),
                        'recommendations': processing_result['action_recommendations']
                    },
                    'acknowledged': False
                }
                
                self.alert_history.append(alert)
                
                # Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡ ÙÙˆØ±ÙŠ Ù„Ù„Ø®Ø·ÙˆØ±Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©
                if risk_level == 'CRITICAL':
                    self._send_critical_alert(alert)
            
            # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ 100 ØªÙ†Ø¨ÙŠÙ‡ Ø­Ø¯ÙŠØ« ÙÙ‚Ø·
            if len(self.alert_history) > 100:
                self.alert_history = self.alert_history[-100:]
                
        except Exception as e:
            self.logger.error(f"âŒ Alert management failed: {e}")
    
    def _send_critical_alert(self, alert: Dict[str, Any]):
        """Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡ Ø­Ø±Ø¬"""
        try:
            # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø£Ø¶Ù Ù‡Ù†Ø§ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª (Email, SMS, etc.)
            self.logger.critical(f"ğŸš¨ CRITICAL ALERT: {alert['message']}")
            self.logger.critical(f"ğŸ“‹ Recommendations: {alert['details']['recommendations']}")
            
        except Exception as e:
            self.logger.error(f"âŒ Critical alert sending failed: {e}")
    
    def train_all_models(self, training_data: List[Dict[str, Any]], 
                        validation_data: List[Dict[str, Any]] = None):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            self.logger.info("ğŸ”„ Starting comprehensive AI models training...")
            
            # ØªØ¯Ø±ÙŠØ¨ Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø°ÙˆØ°
            self.anomaly_system.train_models(training_data)
            
            # ØªØ¯Ø±ÙŠØ¨ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤
            self.prediction_engine.train_models(training_data, validation_data)
            
            self.system_status = 'trained'
            self.logger.info("âœ… All AI models trained successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ Comprehensive training failed: {e}")
            self.system_status = 'training_error'
    
    def get_system_status(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
        return {
            'system_status': self.system_status,
            'anomaly_system_status': self.anomaly_system.get_system_status(),
            'prediction_engine_status': {
                'is_trained': self.prediction_engine.is_trained,
                'active_model': self.prediction_engine.active_model.__class__.__name__ if self.prediction_engine.active_model else 'None',
                'prediction_history_count': len(self.prediction_engine.prediction_history)
            },
            'performance_metrics_summary': {
                'total_processing_count': len(self.performance_metrics.get('processing_time_metrics', [])),
                'average_processing_time': np.mean([m['processing_time'] for m in self.performance_metrics.get('processing_time_metrics', [])]) if self.performance_metrics.get('processing_time_metrics') else 0,
                'critical_alerts_count': len([a for a in self.alert_history if a['level'] == 'CRITICAL']),
                'high_risk_percentage': len([m for m in self.performance_metrics.get('risk_level_metrics', []) if m['risk_level'] in ['HIGH', 'CRITICAL']]) / max(1, len(self.performance_metrics.get('risk_level_metrics', []))) * 100
            },
            'last_updated': datetime.now()
        }

# =============================================================================
# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…ØªÙ‚Ø¯Ù…Ø©
# =============================================================================

def _identify_critical_points(sensor_data: Dict[str, Any], config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø­Ø±Ø¬Ø© ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ´Ø¹Ø±Ø§Øª"""
    critical_points = []
    
    for sensor, value in sensor_data.items():
        sensor_config = config['sensors'].get(sensor, {})
        critical_threshold = sensor_config.get('critical', 100)
        warning_threshold = sensor_config.get('warning', critical_threshold * 0.8)
        
        if value >= critical_threshold:
            critical_points.append({
                'sensor': sensor,
                'value': value,
                'threshold': critical_threshold,
                'severity': 'CRITICAL',
                'impact': 'IMMEDIATE_SAFETY_RISK'
            })
        elif value >= warning_threshold:
            critical_points.append({
                'sensor': sensor,
                'value': value,
                'threshold': warning_threshold,
                'severity': 'WARNING',
                'impact': 'POTENTIAL_SAFETY_RISK'
            })
    
    return critical_points

def _generate_training_data(sensor_history: List[Dict[str, Any]], 
                          sequence_length: int = 50) -> Tuple[np.ndarray, np.ndarray]:
    """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ù† Ø§Ù„Ø³Ø¬Ù„"""
    try:
        if len(sensor_history) < sequence_length + 1:
            return np.array([]), np.array([])
        
        features = []
        targets = []
        
        for i in range(len(sensor_history) - sequence_length):
            # Ù…ØªÙˆØ§Ù„ÙŠØ© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            sequence = sensor_history[i:i + sequence_length]
            feature_vector = []
            
            for data_point in sequence:
                vector = [
                    data_point.get('pressure', 0.0),
                    data_point.get('temperature', 0.0),
                    data_point.get('methane', 0.0),
                    data_point.get('hydrogen_sulfide', 0.0),
                    data_point.get('vibration', 0.0),
                    data_point.get('flow', 0.0)
                ]
                feature_vector.append(vector)
            
            # Ø§Ù„Ù‡Ø¯Ù (Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©)
            target_point = sensor_history[i + sequence_length]
            target_vector = [
                target_point.get('pressure', 0.0),
                target_point.get('temperature', 0.0),
                target_point.get('methane', 0.0),
                target_point.get('hydrogen_sulfide', 0.0),
                target_point.get('vibration', 0.0),
                target_point.get('flow', 0.0)
            ]
            
            features.append(feature_vector)
            targets.append(target_vector)
        
        return np.array(features), np.array(targets)
        
    except Exception as e:
        logging.getLogger('SmartNeural.AI.Training').error(f"Training data generation failed: {e}")
        return np.array([]), np.array([])

def _calculate_advanced_metrics(predictions: np.ndarray, actuals: np.ndarray) -> Dict[str, float]:
    """Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø¯Ø§Ø¡ Ù…ØªÙ‚Ø¯Ù…Ø©"""
    try:
        if len(predictions) == 0 or len(actuals) == 0:
            return {}
        
        # Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        mse = np.mean((predictions - actuals) ** 2)
        mae = np.mean(np.abs(predictions - actuals))
        rmse = np.sqrt(mse)
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø¯Ù‚Ø©
        accuracy = 1.0 / (1.0 + rmse)
        
        # Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ¯ (RÂ²)
        ss_res = np.sum((actuals - predictions) ** 2)
        ss_tot = np.sum((actuals - np.mean(actuals)) ** 2)
        r_squared = 1 - (ss_res / (ss_tot + 1e-8))
        
        # Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ© Ù„Ù„Ø®Ø·Ø£
        mape = np.mean(np.abs((actuals - predictions) / (actuals + 1e-8))) * 100
        
        return {
            'mse': float(mse),
            'mae': float(mae),
            'rmse': float(rmse),
            'accuracy': float(accuracy),
            'r_squared': float(r_squared),
            'mape': float(mape),
            'volatility': float(np.std(predictions - actuals))
        }
        
    except Exception as e:
        logging.getLogger('SmartNeural.AI.Metrics').error(f"Advanced metrics calculation failed: {e}")
        return {}

# =============================================================================
# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# =============================================================================

def create_ai_system(config: Dict[str, Any]) -> AISystemManager:
    """Ø¯Ø§Ù„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    try:
        ai_system = AISystemManager(config)
        logging.getLogger('SmartNeural.AI').info("ğŸ¯ Advanced AI System Created Successfully - SS Rating")
        return ai_system
    except Exception as e:
        logging.getLogger('SmartNeural.AI').error(f"âŒ AI System creation failed: {e}")
        raise

# ØªØµØ¯ÙŠØ± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
__all__ = [
    'AISystemManager',
    'AdvancedAnomalySystem', 
    'AdvancedPredictionEngine',
    'AdaptiveLearningSystem',
    'create_ai_system',
    '_identify_critical_points',
    '_generate_training_data',
    '_calculate_advanced_metrics'
                ]
