import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import logging
from datetime import datetime
from enum import Enum
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒØªØ¨Ø§Øª preprocessing Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
try:
    from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
    from sklearn.impute import KNNImputer
    from sklearn.feature_selection import mutual_info_regression
    from scipy import stats
    HAS_ADVANCED_PREPROCESSING = True
except ImportError:
    HAS_ADVANCED_PREPROCESSING = False

class DataPreprocessor:
    def __init__(self, config):
        self.config = config
    
    def preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        return preprocess_data(data)
    
    def preprocess_realtime_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        return preprocess_realtime_data(data)
        
class DataQualityLevel(Enum):
    RAW = "raw"
    CLEANED = "cleaned"
    ENGINEERED = "engineered"
    NORMALIZED = "normalized"
    READY_FOR_TRAINING = "ready"

@dataclass
class DataQualityReport:
    timestamp: datetime
    original_shape: Tuple[int, int]
    final_shape: Tuple[int, int]
    missing_values: int
    outliers_removed: int
    quality_level: DataQualityLevel
    processing_time_seconds: float
    features_added: List[str]
    features_removed: List[str]

class AdvancedDataPreprocessor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.scalers: Dict[str, Any] = {}
        self.imputers: Dict[str, Any] = {}
        self.feature_importance: Dict[str, float] = {}
        self.quality_reports: List[DataQualityReport] = []
        self.setup_logging()
        
    def setup_logging(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def load_and_preprocess(self, filepath: str, 
                          target_columns: List[str] = None) -> pd.DataFrame:
        """ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„"""
        start_time = datetime.now()
        
        # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.logger.info(f"ğŸ“‚ Loading data from {filepath}")
        raw_df = self._load_data(filepath)
        
        # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        initial_report = self._generate_quality_report(raw_df, DataQualityLevel.RAW)
        
        # 3. Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        cleaned_df = self._clean_data(raw_df)
        
        # 4. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        imputed_df = self._handle_missing_values(cleaned_df)
        
        # 5. Ø¥Ø¶Ø§ÙØ© Features Ù‡Ù†Ø¯Ø³ÙŠØ©
        engineered_df = self._feature_engineering(imputed_df, target_columns)
        
        # 6. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªØ·Ø±ÙØ©
        processed_df = self._handle_outliers(engineered_df)
        
        # 7. Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        normalized_df = self._normalize_data(processed_df)
        
        # 8. ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        processing_time = (datetime.now() - start_time).total_seconds()
        final_report = DataQualityReport(
            timestamp=datetime.now(),
            original_shape=raw_df.shape,
            final_shape=normalized_df.shape,
            missing_values=raw_df.isnull().sum().sum(),
            outliers_removed=len(raw_df) - len(processed_df),
            quality_level=DataQualityLevel.READY_FOR_TRAINING,
            processing_time_seconds=processing_time,
            features_added=list(set(normalized_df.columns) - set(raw_df.columns)),
            features_removed=list(set(raw_df.columns) - set(normalized_df.columns))
        )
        
        self.quality_reports.append(final_report)
        self.logger.info(f"âœ… Data preprocessing completed in {processing_time:.2f}s")
        
        return normalized_df
    
    def _load_data(self, filepath: str) -> pd.DataFrame:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        try:
            if filepath.endswith('.csv'):
                df = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)
            elif filepath.endswith('.parquet'):
                df = pd.read_parquet(filepath)
            elif filepath.endswith('.feather'):
                df = pd.read_feather(filepath)
            else:
                raise ValueError(f"Unsupported file format: {filepath}")
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª
            if df.empty:
                raise ValueError("Loaded dataframe is empty")
                
            self.logger.info(f"ğŸ“Š Loaded data shape: {df.shape}")
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ Error loading data: {e}")
            raise
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""
        cleaned_df = df.copy()
        
        # 1. Ø¥ØµÙ„Ø§Ø­ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        cleaned_df.columns = [col.strip().lower().replace(' ', '_') for col in cleaned_df.columns]
        
        # 2. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        date_columns = [col for col in cleaned_df.columns if 'date' in col or 'time' in col]
        for col in date_columns:
            try:
                cleaned_df[col] = pd.to_datetime(cleaned_df[col])
            except:
                self.logger.warning(f"Could not parse date column: {col}")
        
        # 3. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        columns_to_drop = self.config.get('columns_to_drop', [])
        cleaned_df = cleaned_df.drop(columns=columns_to_drop, errors='ignore')
        
        return cleaned_df
    
    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø°ÙƒØ§Ø¡"""
        imputed_df = df.copy()
        missing_percentage = imputed_df.isnull().mean()
        
        for column in imputed_df.columns:
            if imputed_df[column].isnull().any():
                missing_pct = missing_percentage[column]
                
                if missing_pct > 0.3:  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ©
                    # Ø­Ø°Ù Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø£ÙƒØ«Ø± Ù…Ù† 30%
                    if missing_pct > 0.3:
                        imputed_df = imputed_df.drop(columns=[column])
                        self.logger.info(f"ğŸ—‘ï¸ Dropped column {column} ({missing_pct:.1%} missing)")
                
                elif missing_pct > 0.05 and HAS_ADVANCED_PREPROCESSING:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… KNN Imputer Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
                    try:
                        imputer = KNNImputer(n_neighbors=5)
                        imputed_df[column] = imputer.fit_transform(imputed_df[[column]]).flatten()
                        self.imputers[column] = imputer
                    except Exception as e:
                        self.logger.warning(f"KNN imputation failed for {column}: {e}")
                        imputed_df[column] = imputed_df[column].fillna(imputed_df[column].median())
                
                else:
                    # ØªØ¹ÙˆÙŠØ¶ Ø¨Ø³ÙŠØ·
                    if imputed_df[column].dtype in ['float64', 'int64']:
                        imputed_df[column] = imputed_df[column].fillna(imputed_df[column].median())
                    else:
                        imputed_df[column] = imputed_df[column].fillna(imputed_df[column].mode()[0])
        
        return imputed_df
    
    def _feature_engineering(self, df: pd.DataFrame, target_columns: List[str] = None) -> pd.DataFrame:
        """Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        engineered_df = df.copy()
        
        # 1. Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ø²Ù…Ù†ÙŠØ©
        datetime_columns = [col for col in engineered_df.columns if engineered_df[col].dtype == 'datetime64[ns]']
        for col in datetime_columns:
            engineered_df[f'{col}_hour'] = engineered_df[col].dt.hour
            engineered_df[f'{col}_dayofweek'] = engineered_df[col].dt.dayofweek
            engineered_df[f'{col}_month'] = engineered_df[col].dt.month
            engineered_df[f'{col}_quarter'] = engineered_df[col].dt.quarter
        
        # 2. Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ø¥Ø­ØµØ§Ø¦ÙŠØ©
        numeric_columns = engineered_df.select_dtypes(include=[np.number]).columns.tolist()
        
        for col in numeric_columns:
            # Rolling statistics
            engineered_df[f'{col}_rolling_mean_6'] = engineered_df[col].rolling(6).mean()
            engineered_df[f'{col}_rolling_std_6'] = engineered_df[col].rolling(6).std()
            
            # Difference features
            engineered_df[f'{col}_diff_1'] = engineered_df[col].diff()
            engineered_df[f'{col}_pct_change'] = engineered_df[col].pct_change()
        
        # 3. Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªÙØ§Ø¹Ù„
        if len(numeric_columns) >= 2:
            for i, col1 in enumerate(numeric_columns):
                for col2 in numeric_columns[i+1:]:
                    engineered_df[f'{col1}_{col2}_ratio'] = engineered_df[col1] / (engineered_df[col2] + 1e-8)
                    engineered_df[f'{col1}_{col2}_product'] = engineered_df[col1] * engineered_df[col2]
        
        # 4. Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ target
        if target_columns and HAS_ADVANCED_PREPROCESSING:
            self._calculate_feature_importance(engineered_df, target_columns)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù„Ø§Ù†Ù‡Ø§Ø¦ÙŠØ©
        engineered_df = engineered_df.replace([np.inf, -np.inf], np.nan)
        engineered_df = engineered_df.fillna(0)
        
        return engineered_df
    
    def _calculate_feature_importance(self, df: pd.DataFrame, target_columns: List[str]):
        """Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        try:
            X = df.drop(columns=target_columns).select_dtypes(include=[np.number])
            y = df[target_columns[0]]  # Ø£ÙˆÙ„ target ÙÙ‚Ø·
            
            # Ø­Ø³Ø§Ø¨ Mutual Information
            mi_scores = mutual_info_regression(X, y)
            self.feature_importance = dict(zip(X.columns, mi_scores))
            
        except Exception as e:
            self.logger.warning(f"Feature importance calculation failed: {e}")
    
    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªØ·Ø±ÙØ© Ø¨Ø°ÙƒØ§Ø¡"""
        processed_df = df.copy()
        numeric_columns = processed_df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… IQR Ù…Ø¹ Ø­Ø¯ÙˆØ¯ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¹Ø¯ÙŠÙ„
            Q1 = processed_df[col].quantile(0.25)
            Q3 = processed_df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 2.5 * IQR  # â¬†ï¸ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ³Ø§Ù…Ø­ Ù‚Ù„ÙŠÙ„Ø§Ù‹
            upper_bound = Q3 + 2.5 * IQR
            
            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªØ·Ø±ÙØ© Ø¨Ø­Ø¯ÙˆØ¯ IQR
            processed_df[col] = np.where(processed_df[col] < lower_bound, lower_bound, processed_df[col])
            processed_df[col] = np.where(processed_df[col] > upper_bound, upper_bound, processed_df[col])
        
        return processed_df
    
    def _normalize_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø§Ù†ØªÙ‚Ø§Ø¦ÙŠ"""
        normalized_df = df.copy()
        numeric_columns = normalized_df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            # Ø§Ø®ØªÙŠØ§Ø± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
            if self._is_normal_distribution(normalized_df[col]):
                scaler = StandardScaler()
            else:
                scaler = RobustScaler()  # Ø£ÙØ¶Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
            
            normalized_df[col] = scaler.fit_transform(normalized_df[[col]].values.reshape(-1, 1))
            self.scalers[col] = scaler
        
        return normalized_df
    
    def _is_normal_distribution(self, series: pd.Series) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØªØ¨Ø¹ ØªÙˆØ²ÙŠØ¹Ø§Ù‹ Ø·Ø¨ÙŠØ¹ÙŠØ§Ù‹"""
        try:
            stat, p_value = stats.normaltest(series.dropna())
            return p_value > 0.05  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª p-value > 0.05 ÙØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø·Ø¨ÙŠØ¹ÙŠØ©
        except:
            return False
    
    def _generate_quality_report(self, df: pd.DataFrame, quality_level: DataQualityLevel) -> DataQualityReport:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        return DataQualityReport(
            timestamp=datetime.now(),
            original_shape=df.shape,
            final_shape=df.shape,
            missing_values=df.isnull().sum().sum(),
            outliers_removed=0,
            quality_level=quality_level,
            processing_time_seconds=0,
            features_added=[],
            features_removed=[]
        )
    
    def get_quality_report(self) -> Optional[DataQualityReport]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± ØªÙ‚Ø±ÙŠØ± Ø¬ÙˆØ¯Ø©"""
        if self.quality_reports:
            return self.quality_reports[-1]
        return None
    
    def save_processed_data(self, df: pd.DataFrame, output_path: str) -> bool:
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            if output_path.suffix == '.csv':
                df.to_csv(output_path, index=False)
            elif output_path.suffix == '.parquet':
                df.to_parquet(output_path, index=False)
            elif output_path.suffix == '.feather':
                df.to_feather(output_path)
            
            self.logger.info(f"ğŸ’¾ Saved processed data to {output_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Error saving processed data: {e}")
            return False

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø©
def create_data_preprocessor(config: Dict[str, Any]) -> AdvancedDataPreprocessor:
    return AdvancedDataPreprocessor(config)
    
